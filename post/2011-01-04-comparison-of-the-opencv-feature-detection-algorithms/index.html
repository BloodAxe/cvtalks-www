<!DOCTYPE html>
<html lang="en-us">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Comparison of the OpenCV’s feature detection algorithms</title>
<meta name="description" content="A personal blog about computer vision, AI, machine learning and programmin in OpenCV">
<meta name="generator" content="Hugo 0.18" />
<meta property="og:title" content="Comparison of the OpenCV’s feature detection algorithms" />
<meta property="og:description" content="Introduction
 “In computer vision and image processing the concept of feature detection refers to methods that aim at computing abstractions of image information and making local decisions at every image point whether there is an image feature of a given type at that point or not. The resulting features will be subsets of the image domain, often in the form of isolated points, continuous curves or connected regions.”" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/2011-01-04-comparison-of-the-opencv-feature-detection-algorithms/" />


<meta property="og:updated_time" content="2011-01-04T00:00:00&#43;00:00"/>










<link rel="dns-prefetch" href="//fonts.googleapis.com" />

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,700,600" type="text/css" media="all" />
<link rel="stylesheet" href="/css/style.css" type="text/css" media="all" />
<script type="text/javascript" src="/js/scripts.js"></script>
<!--[if lt IE 9]>
	<script src="/js/css3-mediaqueries.js"></script>
<![endif]-->
</head>
<body class="home blog mr-right-sb mr-mobile" itemscope="itemscope" itemtype="http://schema.org/WebPage">
	<div class="mr-container mr-container-outer">
		<div class="mr-header-mobile-nav clearfix"></div>
			<header class="mr-header" itemscope="itemscope" itemtype="http://schema.org/WPHeader">
				<div class="mr-container mr-container-inner mr-row clearfix">
					<div class="mr-custom-header clearfix">
						<div class="mr-site-identity">
							<div class="mr-site-logo" role="banner" itemscope="itemscope" itemtype="http://schema.org/Brand">
								<div class="mr-header-text">
									<a class="mr-header-text-link" href="/" title="Computer Vision Talks" rel="home">
										<h1 class="mr-header-title">Computer Vision Talks</h1>
										<h2 class="mr-header-tagline">All you want to know about image processing</h2>
									</a>
								</div>
							</div>
						</div>
					</div>
				</div>
				<div class="mr-main-nav-wrap">
					<nav class="menu" itemscope="itemscope" itemtype="http://schema.org/SiteNavigationElement">
	<ul class="menu__list">
		
		
		
			<li class="menu__item "><a class="menu__link" href="/about/">ABOUT ME</a></li>
		
		
	</ul>
</nav>
				</div>
			</header>
		<div class="mr-wrapper clearfix">


<div class="main-content mr-content" role="main" itemprop="mainContentOfPage">
	<article class="post">
		<header class="entry-header clearfix">
			<h1 class="entry-title">Comparison of the OpenCV’s feature detection algorithms</h1>
			<p class="mr-meta entry-meta">
				<svg class="icon icon-time" height="14" viewBox="0 0 16 16" width="14" xmlns="http://www.w3.org/2000/svg"><path d="m8-.0000003c-4.4 0-8 3.6-8 8 0 4.4000003 3.6 8.0000003 8 8.0000003 4.4 0 8-3.6 8-8.0000003 0-4.4-3.6-8-8-8zm0 14.4000003c-3.52 0-6.4-2.88-6.4-6.4000003 0-3.52 2.88-6.4 6.4-6.4 3.52 0 6.4 2.88 6.4 6.4 0 3.5200003-2.88 6.4000003-6.4 6.4000003zm.4-10.4000003h-1.2v4.8l4.16 2.5600003.64-1.04-3.6-2.1600003z"/></svg>
				<time class="entry-meta-date updated" datetime="2011-01-04 00:00:00 &#43;0000 UTC">January 04, 2011</time>
			</p>
		</header>
		<div class="entry-content clearfix">
			
			

<p><strong>Introduction</strong></p>

<blockquote>
<p>“In computer vision and image processing the concept of <strong>feature detection</strong> refers to methods that aim at computing abstractions of image information and making local decisions at every image point whether there is an image feature of a given type at that point or not.
The resulting features will be subsets of the image domain, often in the form of isolated points, continuous curves or connected regions.”</p>
</blockquote>

<p>Wikipedia</p>

<p><span class="more"></span></p>

<p><strong>Update</strong>: Here is <a href="http://computer-vision-talks.com/2011/07/comparison-of-the-opencvs-feature-detection-algorithms-ii/">Updated Post</a> with added new detectors (ORB and SIFT).    </p>

<h2 id="overview">Overview</h2>

<p>OpenCV is free open-source library intended for use in image processing, computer vision and machine learning areas. It have a huge amount of different algorithms, but in this topic i will compare their existing feature detectors. At this moment OpenCV has stable 2.2 version and following types of descriptors: <strong>Fast</strong>, <strong>GoodFeaturesToTrack</strong>, <strong>Mser</strong>, <strong>Star</strong>, <strong>Sift</strong>, <strong>Surf</strong>. And few Adapters over detectors: GridAdapted, PyramidAdapted, DynamicAdapted. In this article noticed above detectors will be compared by speed, quality and repeatability in different lighting and scale.</p>

<h2 id="test-images">Test images</h2>

<p> <img src="mandril_thumb.jpg" alt="mandril" title="mandril" /><img src="barbara_thumb.jpg" alt="barbara" title="barbara" /> <img src="lena_thumb.jpg" alt="lena" title="lena" /><img src="peppers_thumb.jpg" alt="peppers" title="peppers" /></p>

<p>All images are 512x512 size. Processing was done on grayscale images.</p>

<h2 id="criteria">Criteria</h2>

<p><strong>Speed</strong> – the most important criteria for real-time image processing. To achieve smooth real time processing your code must process single frame in less than 30 milliseconds! It’s quite enough for four cores of Core i7 processor, but imagine your code will be running on mobile device, which have ten times lower computation power. We will test how fast our detectors are on the laptop CPU and on the mobile processor.</p>

<p>Test case: Detect feature points on the given image and measure spent time.</p>

<p><strong>Quality</strong> – detected features usually used in further tracking or matching (for SLAM, panorama stitching, object detection, etc..). So “feature consumer” expects that detected features are good enough for it. Detectors will be tested for Optical Flow KLT tracking and two-frames matching using SIFT and SURF descriptors. For tracking test I will apply some affine transformation for original image and detected on the previous step keypoints. With transformed image and transformed keypoints we can estimate quality of tracking.</p>

<p>Here is screenshots demonstrating transformed images and tracks of the features.</p>

<p><img src="Mandril-GFTT-Tracking_thumb.jpg" alt="Mandril-GFTT-Tracking" title="Mandril-GFTT-Tracking" /><img src="Barbara-SURF-Tracking_thumb.jpg" alt="Barbara-SURF-Tracking" title="Barbara-SURF-Tracking" /><img src="Lena-FAST-Tracking_thumb.jpg" alt="Lena-FAST-Tracking" title="Lena-FAST-Tracking" /><img src="Peppers-FAST-Tracking_thumb.jpg" alt="Peppers-FAST-Tracking" title="Peppers-FAST-Tracking" /></p>

<p>**Lighting and scale invariant – **feature detectors are expected to detect the same features either on large or small images of one object. That’s true also for lighting – slight brightness and contrast fluctuations shouldn’t affect on the feature detector significantly. Almost all modern cameras have automatic gain control which automatically adjust exposure trying to avoid over- or underexposed areas in image. Robust detection is critical for further processing.</p>

<p>Here is explanation screenshot:</p>

<p><img src="Barbara-STAR-Matching_thumb.jpg" alt="Barbara-STAR-Matching" title="Barbara-STAR-Matching" /><img src="Peppers-GFTT-Pyramid-Matching_thumb.jpg" alt="Peppers-GFTT Pyramid-Matching" title="Peppers-GFTT Pyramid-Matching" /></p>

<p>As you can see, the second picture for each pair are slightly brighter. This test simulates automatic camera gain option for many modern web-cams.</p>

<h2 id="estimation-marks">Estimation marks</h2>

<p><strong>Speed per frame</strong> – absolute total time in milliseconds spent to the feature detection of the single frame.</p>

<p><strong>Speed per keypoint</strong> – detection time for single keypoint. Evaluated as total time divided to number of detected keypoints. Helps us to estimate how cheap the detection actually is.</p>

<p><strong>Percent of tracked features</strong> – percent of successfully tracked features from original to transformed image. In ideal situation, value of this mark should be near 100%.</p>

<p><strong>Average tracking error</strong> – this is the average distance between position of tracked feature and their calculated position on transformed frame. This mark indicates accuracy of the feature detection. Large values indicates large number of false positive tracking or “drift” of feature point among frames.</p>

<p><strong>Features count deviation</strong> – difference between number of keypoints on reference frame and number of detected keypoints on transformed frame divided by number of keypoints on reference frame. Helps estimate how slight exposure changes affects feature detection.</p>

<p><strong>Average detection error</strong> – average distance between nearest keypoints on original and transformed frame.</p>

<h2 id="testing-hardware-configuration">Testing hardware configuration</h2>

<p>We will test all set of feature detectors on two configurations – laptop and mobile on the same data sets to compare performance and ensure that detection results are identical for both platforms.</p>

<p> 
<strong>MacBook Pro</strong>
<strong>iPhone 3GS</strong></p>

<p>CPU Model
Core 2 Duo T7400
ARM Cortex A8</p>

<p>CPU Clock
2160 MHz
600 MHz</p>

<p>CPU Cores number
2
1</p>

<h2 id="results">Results</h2>

<p>We start our analysis from number of detected feature points. And on the chart below we see that FAST detector finds thousands of keypoints, while other detectors finds only hundreds. Unlike the other detectors, keypoints  detected by FAST can contain too many “noise” features – which are not appropriate for further tracking.<img src="Number-of-detected-features_thumb.png" alt="Number of detected features" title="Number of detected features" /></p>

<p>Speed of feature detection will be examined using two criteria&rsquo;s – by total amount of time spent for the detection of keypoint on the whole frame and a time per single keypoint which is simply total time divided by number of detected keypoints. As expected, FAST detector provides best detection time per feature.</p>

<p><img src="Detection-cost_thumb.png" alt="Detection cost" title="Detection cost" /></p>

<p>Finally the last performance tests – how fast are feature detectors on mobile devices? Here is four charts (for four different images) that will answer this question:</p>

<p><img src="Feature-detection-comparison-peppers_thumb.png" alt="Feature detection comparison - peppers" title="Feature detection comparison - peppers" /></p>

<p><img src="Speed-comparision-barbara_thumb.png" alt="Speed comparision - barbara" title="Speed comparision - barbara" /></p>

<p><img src="Speed-comparison-lena_thumb.png" alt="Speed comparison - lena" title="Speed comparison - lena" /></p>

<p><img src="Speed-comparison-mandrill_thumb.png" alt="Speed comparison - mandrill" title="Speed comparison - mandrill" /></p>

<p>As we can see from charts above - on the mobile platform all feature detectors works ~3-5 times slower than on desktop. So without optimization for architecture of concrete processor real-time performance can’t be achieved (see conclusion).</p>

<p>Now to quality estimation. Following chart demonstrates average tracking error (in pixels) between reference frame and slightly transformed. Pyramid KLT tracker was used (cvCalcOpticalFlowPyrLK). STAR (Also known as CenSurE) detector found keypoints that were tracked best than other detectors.</p>

<p><img src="Average-tracking-error_thumb.png" alt="Average tracking error" title="Average tracking error" /></p>

<p>Chart below  gives us information about how many of features were tracked successfully. All detectors provide very good feature points, except for MSER detectors, which score is less that other’s detectors for all test images.</p>

<p><img src="Percent-of-tracked-feature_thumb.png" alt="Percent of tracked feature" title="Percent of tracked feature" /></p>

<p>A last chart shows how feature detectors co-variant with brightness changes. STAR and MSER detectors are very sensible to brightness variation, while the other detectors are not.</p>

<p><img src="Average-feature-point-drift_thumb.png" alt="Average feature point drift" title="Average feature point drift" /></p>

<p> </p>

<h2 id="conclusion">Conclusion</h2>

<p>The choice of the feature detector very much depends on a problem. For example, if you doing monocular SLAM, FAST detector is weapon of choice, because it’s fast and detects a lot of features. For image stitching, pattern recognition and other feature-descriptor related tasks, scale- and rotation-invariant detectors are preferable.</p>

<p>For desktop applications almost all feature detectors guarantee real-time (25 fps+) performance. But for mobile device the fastest detector (FAST) works only with ~10fps. Why such huge difference? First of all, laptop CPU is much more powerful. But also, cache size and processor architecture does matter. OpenCV supports SSE/SSE2/SSE3 intrinsic instructions which can deal 10x speedup because of instruction vectorization. Unfortunately, OpenCV supports intrinsic instructions only for x86 architecture. There are ARM NEON SIMD engine, and theoretically with their help we can significantly improve performance of feature detection on mobile platforms. But this require a lot of experience and low-level knowledge of CPU architecture. I will reveal benefits of the NEON intrinsic in my further posts.</p>

<p>This article highlights only few performance and quality aspects of the feature detection problem. I highly recommend you to read the article “<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.4126&amp;rep=rep1&amp;type=pdf">Local Invariant Feature Detectors: A Survey</a>” the get much more knowledge of how different feature detectors work.</p>

		</div>
		
<div class="entry-tags clearfix">
	<svg class="icon icon-tag" width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><path d="m4.73135 3.3795002q0-.5597-.39604-.9557-.39604-.3961-.95577-.3961-.55974 0-.95578.3961-.39604.396-.39604.9557 0 .5598.39604.9558.39604.3961.95578.3961.55973 0 .95577-.3961.39604-.396.39604-.9558zm11.26865 6.0832q0 .5596998-.39076.9504998l-5.18548 5.196q-.41188.3908-.9610504.3908-.55974 0-.9505-.3908l-7.5511496-7.5616998q-.40132-.3907-.68119-1.0666-.27987-.6759-.27987-1.2357v-4.3934q0-.54920004.40132-.95050004.40132-.4013.9505-.4013h4.39339q.55974 0 1.23565.2799.67591.2798 1.07723.6812l7.55115 7.54060004q.39076.4118.39076.961z"/></svg>
	<ul>
		<li><a href="/tags/opencv/" rel="tag">opencv</a></li>
		
	</ul>
</div>
	</article>
	
<div class="mr-author-box clearfix">
	<figure class="mr-author-box-avatar">
		<img alt="Eugene Khvedchenya avatar" src="/img/avatar.png" class="avatar avatar-90 photo" height="90" width="90">
	</figure>
	<div class="mr-author-box-header">
		<span class="mr-author-box-name">About Eugene Khvedchenya</span>
	</div>
	<div class="mr-author-box-bio">
		Eugene Khvedchenya is a computer vision developer skilled in high-performance image processing. In his spare time he loves to share his knowledge and thoughts about computer vision and augmented reality
	</div>
</div>

	

	<nav class="mr-post-nav mr-row clearfix" itemscope="itemscope" itemtype="http://schema.org/SiteNavigationElement">
		
		<div class="mr-col-1-2 mr-post-nav-item mr-post-nav-prev">
			<a href="/post/2011-01-12-using-opencv-in-objective-c-code/" rel="prev"><span>«Previous</span><p>Using OpenCV in Objective-C code</p></a>
		</div>
		
		
		<div class="mr-col-1-2 mr-post-nav-item mr-post-nav-next">
			<a href="/post/2010-12-30-building-opencv-for-ios/" rel="next"><span>Next»</span><p>Building OpenCV for iOS</p></a>
		</div>
		
	</nav>


	
<div id="mr-comments" class="mr-comments-wrap">
	<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'cvtalks';
    var disqus_identifier = '\/post\/2011-01-04-comparison-of-the-opencv-feature-detection-algorithms\/';
    var disqus_title = 'Comparison of the OpenCV’s feature detection algorithms';
    var disqus_url = '\/post\/2011-01-04-comparison-of-the-opencv-feature-detection-algorithms\/';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

</div>

<aside class="mr-sidebar" itemscope="itemscope" itemtype="http://schema.org/WPSideBar">
	
<div class="mr-widget widget_search">
	<form class="search-form" role="search" method="get" action="//google.com/search">
		<label>
			<span class="screen-reader-text">Search for:</span>
			<input type="search" class="search-field" placeholder="SEARCH..." value="" name="q">
		</label>
		<input class="search-submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="/" />
	</form>
</div>

	
<div class="mr-widget widget_recent_entries">
	<h4 class="mr-widget-title"><span class="mr-widget-title-inner">Recent Posts</span></h4>
	<ul>
		
		<li><a href="/post/how-to-write-good-code/">How to write a good code</a></li>
		
		<li><a href="/post/introducing-cloudcv-bootstrap/">Introducing CloudCV bootstrap</a></li>
		
		<li><a href="/post/how-to-debug-nodejs-addons-in-visual-studio/">How to debug node.js addons in Visual Studio</a></li>
		
		<li><a href="/post/hacking-opencv-for-fun-and-profit/">Hacking OpenCV for fun and profit</a></li>
		
		<li><a href="/post/tile-based-image-processing/">Tile-based image processing</a></li>
		
		<li><a href="/post/image-processing-in-your-browser-unit-test-automation/">Image processing in your browser - Unit Test automation</a></li>
		
		<li><a href="/post/computer-vision-digest-september-2014/">Computer Vision Digest - September 2014</a></li>
		
		<li><a href="/post/how-to-convert-args-from-js-to-cpp/">Argument checking for native addons for Node.js. Do it right!</a></li>
		
		<li><a href="/post/computer-vision-digest-august-2014/">Computer Vision Digest - August 2014</a></li>
		
		<li><a href="/post/mapping-eigen-to-opencv/">Mapping data from Eigen to OpenCV and back</a></li>
		
	</ul>
</div>


	




	


<div class="mr-widget widget_tag_cloud">
	<h4 class="mr-widget-title"><span class="mr-widget-title-inner">Tags</span></h4>
	<div class="tagcloud">
		
			<a href="/tags/algorithms" class="tag-link" title="algorithms" style="font-size: 12px;">algorithms</a>
		
			<a href="/tags/ar" class="tag-link" title="ar" style="font-size: 12px;">ar</a>
		
			<a href="/tags/books" class="tag-link" title="books" style="font-size: 12px;">books</a>
		
			<a href="/tags/cloud-computing" class="tag-link" title="cloud-computing" style="font-size: 12px;">cloud-computing</a>
		
			<a href="/tags/cloudcv" class="tag-link" title="cloudcv" style="font-size: 12px;">cloudcv</a>
		
			<a href="/tags/coursera" class="tag-link" title="coursera" style="font-size: 12px;">coursera</a>
		
			<a href="/tags/cpp" class="tag-link" title="cpp" style="font-size: 12px;">cpp</a>
		
			<a href="/tags/digest" class="tag-link" title="digest" style="font-size: 12px;">digest</a>
		
			<a href="/tags/iphone" class="tag-link" title="iphone" style="font-size: 12px;">iphone</a>
		
			<a href="/tags/neon" class="tag-link" title="neon" style="font-size: 12px;">neon</a>
		
			<a href="/tags/news" class="tag-link" title="news" style="font-size: 12px;">news</a>
		
			<a href="/tags/nodejs" class="tag-link" title="nodejs" style="font-size: 12px;">nodejs</a>
		
			<a href="/tags/opencv" class="tag-link" title="opencv" style="font-size: 12px;">opencv</a>
		
			<a href="/tags/reddit" class="tag-link" title="reddit" style="font-size: 12px;">reddit</a>
		
			<a href="/tags/scala" class="tag-link" title="scala" style="font-size: 12px;">scala</a>
		
			<a href="/tags/tutorials" class="tag-link" title="tutorials" style="font-size: 12px;">tutorials</a>
		
			<a href="/tags/visualstudio" class="tag-link" title="visualstudio" style="font-size: 12px;">visualstudio</a>
		
			<a href="/tags/xcode" class="tag-link" title="xcode" style="font-size: 12px;">xcode</a>
		
	</div>
</div>



</aside>
	</div>
		<div class="mr-copyright-wrap">
			<div class="mr-container mr-container-inner clearfix">
				<p class="mr-copyright">&copy; 2017 Computer Vision Talks. Based on <a href="//wordpress.org/themes/mh-magazine-lite/" target="_blank" rel="nofollow noopener noreferrer">MH Magazine lite</a>.</p>
			</div>
		</div>
	</div>

<script>
	var navigation = responsiveNav(".menu", {
		navClass: "menu--collapse",
	});
</script>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-10687369-5', 'auto');
ga('send', 'pageview');
</script>

</body>
</html>