<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Computer Vision Talks</title>
    <link>/post/index.xml</link>
    <description>Recent content in Posts on Computer Vision Talks</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Sep 2015 00:00:00 +0000</lastBuildDate>
    <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How to write a good code</title>
      <link>/post/how-to-write-good-code/</link>
      <pubDate>Wed, 09 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/how-to-write-good-code/</guid>
      <description>

&lt;div class=&#34;featured-image&#34;&gt;
![](featured-image.jpg)
&lt;/div&gt;

&lt;p&gt;This article is a quintessence of my all experience
I&amp;rsquo;ve got for last years working as a computer vision consultant.
I hope you will find this interesting and useful.
My goal was to create set of rules I follow personally on daily basis.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;1-prefer-functional-approach&#34;&gt;1. Prefer functional approach&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;fp.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Image processing is a place where functional paradigm shows it&amp;rsquo;s bests.
In most cases, image processing algorithm depends only on input image and has no side effects.
This fits perfectly to a &amp;lsquo;pure function&amp;rsquo; term. When possible try to follow this checklist when you define a function in your code:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mark all input data with &lt;code&gt;const&lt;/code&gt; modifier to specify immutable arguments.&lt;/li&gt;
&lt;li&gt;Prefer return by reference for large objects (especially for images) instead returning by value.&lt;/li&gt;
&lt;li&gt;In case of class methods, mark methods that does not change class internal state with  &lt;code&gt;const&lt;/code&gt; modifier.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These simple advice helps to understand what and when can you function change. You may remember tricky details of your code today, but who guarantees you&amp;rsquo;ll easily remember that in a month?&lt;/p&gt;

&lt;p&gt;For instance, I want to write implementation of template matching. One may write it as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;class TemplateMatchingAlgorithm
{
public:
  TemplateMatchingAlgorithm(cv::Mat templateImage, int method);

  cv::Point matchTemplate(cv::Mat queryImage) const;

private:
  cv::Mat _templateImage;
  int   _method;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare it with function declaration that does the same job, but looks much cleaner:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void MatchTemplate(cv::Mat templateImage, cv::Mat queryImage, cv::Point&amp;amp; minPoint, int method);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So you may ask, should I create class with const method or declare an ordinary function instead?
The short answer - functions are better. I personally use simple decision algorithm:&lt;/p&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
  If the algorithm needs to preserve state between calls - use class; otherwise - use function.
&lt;/div&gt;

&lt;h2 id=&#34;2-don-t-use-virtual-methods&#34;&gt;2. Don&amp;rsquo;t use virtual methods&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;virtualmethods.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You may argue - with classes we can define various implementations for &lt;code&gt;TemplateMatching&lt;/code&gt; using SIDM, CUDA or use template matching in Fourier domain.
Yes, we can. But the price we pay for each call of virtual method is too big for such small routine as template matching.
Usually we use TemplateMatching on small patches like 11x11 pixels to track translation between two frames of video. Hence to achieve robust tracking, number of patches can be quite high - 500 and even 1000 per one frame. Further, coarse-to-fine matching and sub-pixel optimization can lead to ten or more calls for the same feature. In this case, virtual call is a big no-no that will kill your application&amp;rsquo;s performance.&lt;/p&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
As a rule of thumb: you *may* use virtual methods to execute big amount of work. Let&#39;s say one virtual call per frame looks totally fine. A thousand calls per frame is obviously a bad, bad idea.
&lt;/div&gt;

&lt;h2 id=&#34;3-write-regression-tests&#34;&gt;3. Write regression tests&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;regression.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Regression testing is a great tool to track all changes in your algorithm and measure it&amp;rsquo;s
precision and performance. Here&amp;rsquo;s an idea:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Create a ground-truth input dataset&lt;/li&gt;
&lt;li&gt;Process it with your algorithm.&lt;/li&gt;
&lt;li&gt;Save output data and track it in your version control system.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Each time you make changes in implementation - run regression on same input data and compare results.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Regression testing can easily spot numeric stability problems on different compilers / platforms, introduced bugs, platform-dependent optimizations. It&amp;rsquo;s a good idea to include it as a part of regular unit testing:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;/*
BOOST_AUTO_TEST_CASE(MyAlgorithm, createRegressionDatabaset)
{
    ...
}
/**/

BOOST_AUTO_TEST_CASE(MyAlgorithm, checkRegression)
{
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I intentionally commented out first test case - in ideal world it should be executed only once.
But sometimes it&amp;rsquo;s necessary to update ground-truth (you fixed a bug in original implementation).
So you uncomment it, run tests, comment it back and check-in new ground-truth.&lt;/p&gt;

&lt;p&gt;You may use any format you like for dumping ground truth data (usually it&amp;rsquo;s some matrices, vectors or images).
Personally, I prefer YAML and JSON.
Just ensure when dumping floating-point numbers to specify maximum output precision.
Otherwise you will have funny weekend debugging absolutely correct algorithm with failing assertion check &lt;code&gt;
0.1543642342365 != 0.154364&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
Once written, regression tests should be run on regular basis either manually or using automated CI system of your choice. 
&lt;/div&gt;

&lt;h2 id=&#34;4-add-logging-to-your-code&#34;&gt;4. Add logging to your code&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;logging.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the simplest case, it could be trivial console logging.
In debug mode you will have all messages in stdout, but in release it will be totally excluded from compilation step.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#if _DEBUG
#define LOG_MESSAGE(x) std::cout &amp;lt;&amp;lt; __FILE__ &amp;lt;&amp;lt; &amp;quot; (&amp;quot; &amp;lt;&amp;lt; __LINE__ &amp;lt;&amp;lt; &amp;quot;): &amp;quot; &amp;lt;&amp;lt; x &amp;lt;&amp;lt; std::endl;
#else
#define LOG_MESSAGE(x)
#endif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For complex systems I suggest to use mature logging frameworks like Boost::Log or similar.
They has separation of logging streams (info, trace, warning, errors) and deal with multi-threaded logging.
Logging to file is also useful feature when you want to store program output for further analysis.&lt;/p&gt;

&lt;p&gt;In one of my previous projects, there was a standalone program for logs analysis and data visualization. We logged
all - matrices, vectors regular messages with timestamps. After program finishes we were able to trace program flow
frame by frame and analyze how our algorithms behaved. I cannot count how much hours this tool saved to us on data analysis.&lt;/p&gt;

&lt;p&gt;Logging also helps to spot nasty bugs when you have inconsistent behavior on different platforms. For instance, not so recently I faced a problem when optical flow tracker gave different results on iOS and OSX platforms. After logging all input/output and intermediate data including vectors, matrices I found the root of the evil. It was &lt;code&gt;std::log&lt;/code&gt; function.&lt;/p&gt;

&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
On OSX ``std::log(float)`` implicitly computes logarithm with double precision and returns truncated result (float). On iOS it computes logarithm using single precision leading to small difference in result. Like a butterfly effect, it affects all other parts of the algorithm. 
&lt;br&gt;
**Without logging it would be practically impossible to spot bug like this**.
&lt;/div&gt;

&lt;h2 id=&#34;5-profile-your-code&#34;&gt;5. Profile your code&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;profilerdump.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Algorithm performance usually a top-level priority since this kind of applications deal with real-time video processing and processing of huge amount of data.
Therefore it&amp;rsquo;s crucial to know how fast your algorithms runs or do they become slower or faster with refactoring you perform.
There are plenty of ways to collect this data.&lt;/p&gt;

&lt;h3 id=&#34;xcode-instruments&#34;&gt;XCode Instruments&lt;/h3&gt;

&lt;p&gt;If you&amp;rsquo;re targeting on OSX and iOS platform, Apple Xcode and Instruments can be your first choice due to natural integration of profiling tools to IDE.
Instruments can be handy to spot problematic places in your code. But Instruments uses sampling technique, which is not precise.&lt;/p&gt;

&lt;h3 id=&#34;vtune-visualstudio&#34;&gt;VTune/VisualStudio&lt;/h3&gt;

&lt;p&gt;For Windows users Visual Studio offers integrated profiler as well.
Unlike Instruments, it can do instrumentation of your binary.
It means each function in your program modified with special prolog and epilog code that measure execution time of all your program.
Instrumenting provides you a lot of information per each routine: calls count, execution time, inclusive / exclusive CPU time, call tread and CPU cores load.
This is much more you have with Apple Instruments.&lt;/p&gt;

&lt;h3 id=&#34;cv-gettickcount&#34;&gt;cv::getTickCount&lt;/h3&gt;

&lt;p&gt;Sometimes you don&amp;rsquo;t want to profile entire application. Instead you want to &amp;lsquo;cherry-pick&amp;rsquo; only a single function and profile it. For this purpose you can use monotonic clock and measure execution time:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#define MEASURE_TIME(x)                        \
        { auto startTime = cv::getTickCount(); \ 
          x;                                   \
          auto endTime = cv::getTickCount();   \
          std::cout &amp;lt;&amp;lt; #x &amp;lt;&amp;lt; &amp;quot; &amp;quot; &amp;lt;&amp;lt; (endTime - startTime) * cv::getTickFrequency() &amp;lt;&amp;lt; std::endl; }

// Measure MatchTemplate
MEASURE_TIME(MatchTemplate(a,b,result));
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
  Profile your code. Always.
&lt;/div&gt;

&lt;h2 id=&#34;6-optimize-code&#34;&gt;6. Optimize code&lt;/h2&gt;

&lt;h3 id=&#34;6-1-loop-vectorization&#34;&gt;6.1 Loop vectorization&lt;/h3&gt;

&lt;p&gt;Compilers can do loops vectorization when data flow and iterations count are clear enough.
This heuristic analysis depends on implementation, so CLang has different vectorization analysis engine than MSVC. But you can give your compiler a hint:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void SSD(cv::Mat i1, cv::Mat i2)
{
  int i = 0;
  const uint8_t * a = templateImage.data;
  const uint8_t * b = templateImage.data;
  
  int ssd = 0;

  for (; i &amp;lt; (length/4)*4; i+=4)
  {
    ssd += SQR(a[i+0] - b[i+0]);
    ssd += SQR(a[i+1] - b[i+1]);
    ssd += SQR(a[i+2] - b[i+2]);
    ssd += SQR(a[i+3] - b[i+3]);
  }

  for (; i &amp;lt; length; i++, a++, b++)
  {
    ssd += SQR(a[i] - b[i]);
  }

  return ssd;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This partial loop unrolling gives enough information to compiler.
As a result it can replace partially unrolled summation with SIMD instruction.&lt;/p&gt;

&lt;h3 id=&#34;6-2-bring-constants-at-compile-time&#34;&gt;6.2 Bring constants at compile time&lt;/h3&gt;

&lt;p&gt;If you have a priory knowledge on size of data you pass to particular function, it may make sense to write function that
employs this information:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template &amp;lt;typename TOut, typename TIn, int RowsAtCompileTime, int ColsAtCompileTime&amp;gt;
inline TOut SSD(const cv::Matx_&amp;lt;TIn, RowsAtCompileTime, ColsAtCompileTime&amp;gt;&amp;amp; a, 
                const cv::Matx_&amp;lt;TIn, RowsAtCompileTime, ColsAtCompileTime&amp;gt;&amp;amp; b) nothrow
{
  int i = 0;
  const TIn * a = templateImage.data;
  const TIn * b = templateImage.data;
  
  TOut ssd = 0;

  for (int i = 0; i &amp;lt; RowsAtCompileTime * ColsAtCompileTime; i++, a++, b++)
  {
    ssd += (TOut)SQR(a[i] - b[i]);
  }

  return ssd;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since compiler knows size of the array to process, it can easily generate vectorized code for this routine.
The drawback of this approach is slightly increased code size if you instantiate this template function with many sizes.
But you get better performance which usually worth it.&lt;/p&gt;

&lt;h3 id=&#34;6-3-architecture-dependent-implementations&#34;&gt;6.3 Architecture-dependent implementations&lt;/h3&gt;

&lt;p&gt;Architecture-specific features like SIMD instructions can make your code runs much, much faster than generic C++
implementation.
It is a must-have feature on mobile platforms since it makes your code faster and at the same time it
conservate battery power of host device.
There are more and more devices with CUDA and OpenCL support.
And the question is - how do I manage all those possible architecture / platforms combinations of optimized functions in my code?&lt;/p&gt;

&lt;p&gt;Here it&amp;rsquo;s how I solved this task for myself:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;namespace mypublicnamespace
{
    void MatchTemplate(cv::Mat templateImage, cv::Mat queryImage, cv::Point&amp;amp; minPoint, int method)
    {
#if TARGET_PLATFORM_HAS_NEON_SIMD
        details::neon::MatchTemplate(templateImage, queryImage, minPoint, method);
#elif TARGET_PLATFORM_HAS_SSE_SIMD
        details::sse::MatchTemplate(templateImage, queryImage, minPoint, method);
#elif TARGET_PLATFORM_HAS_OPENCL
        details::opencl::MatchTemplate(templateImage, queryImage, minPoint, method);
#else        
        details::generic::MatchTemplate(templateImage, queryImage, minPoint, method);
#endif    
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code snippet demonstrate compile-time dispatching for particular implementation of a function declared in &lt;code&gt;mypublicnamespace&lt;/code&gt;. Of course, you should take care of preprocessor defines that declare platform / architecture capabilities. I&lt;/p&gt;

&lt;h3 id=&#34;6-4-branch-prediction&#34;&gt;6.4 Branch prediction&lt;/h3&gt;

&lt;p&gt;Suppose you have a-priory knowledge that condition expression will be almost always true.
Why don&amp;rsquo;t give this intrinsic knowledge to compiler? By supplying &lt;em&gt;expected&lt;/em&gt; condition result compiler can
generate more efficient code. As a result, CPU will start decoding instructions earlier.&lt;/p&gt;

&lt;p&gt;Unfortunately, this feature supported only on GCC and CLANG.
But according to measurements, it can provide significant speed-up up to ~15%. You can find more information here: &lt;a href=&#34;http://blog.man7.org/2012/10/how-much-do-builtinexpect-likely-and.html&#34;&gt;How much do __builtin_expect(), likely(), and unlikely() improve performance?&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#define LIKELY(x)      __builtin_expect(!!(x), 1)
#define UNLIKELY(x)    __builtin_expect(!!(x), 0)

if (LIKELY(x &amp;gt;= 0 &amp;amp;&amp;amp; x &amp;lt;= image_width))
{
  // Compute something
}

if (UNLIKELY(std::fabs(value) &amp;lt;= std::numeric_limits&amp;lt;float&amp;gt;::epsilon()))
{
  throw std::runtime_error(&amp;quot;Value is zero&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;6-5-openmp&#34;&gt;6.5 OpenMP&lt;/h3&gt;

&lt;p&gt;Starting from OpenMP 4.0, you can instruct compiler to generate vectorized code by adding new pragma instructions to your loops:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void MatchTemplate(cv::Mat templateImage, cv::Mat queryImage, cv::Point&amp;amp; minPoint, int method)
{
  uint8_t * a = templateImage.data;
  uint8_t * b = templateImage.data;
  
  int ssd = 0;

#pragma omp simd reduction(+:x)
  for (int i = 0; i &amp;lt; length; i++)
  {
    ssd += SQR(a[0] - b[0]);
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With only single &lt;code&gt;#pragma&lt;/code&gt; instruction you made your code runs faster.
I encourage you to visit &lt;a href=&#34;https://software.intel.com/en-us/articles/enabling-simd-in-program-using-openmp40&#34;&gt;Enabling SIMD in program using OpenMP4.0&lt;/a&gt; webpage for more information of supported OpenMP SIMD instructions.&lt;/p&gt;

&lt;h3 id=&#34;7-use-imageview&#34;&gt;7. Use ImageView&lt;/h3&gt;

&lt;p&gt;For Windows users there is a great Visual Studio plugin called &lt;a href=&#34;https://visualstudiogallery.msdn.microsoft.com/e682d542-7ef3-402c-b857-bbfba714f78d&#34;&gt;ImageWatch&lt;/a&gt; that makes our life so simple.
This plugin can visualize OpenCV matrices right in IDE.
It is hard to overestimate the usefulness of this plugin.
You can see how images are changing while debugging.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image_watch.png&#34; alt=&#34;Image watch&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Next time when you start development of new algorithm, keep in mind these simple steps.
They will help you create fast, maintainable and clear code. Here they are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Prefer functional approach&lt;/li&gt;
&lt;li&gt;Try avoid virtual calls&lt;/li&gt;
&lt;li&gt;Write vectorization-friendly code&lt;/li&gt;
&lt;li&gt;Use all available debugging / profiling tools&lt;/li&gt;
&lt;li&gt;Measure your code performance&lt;/li&gt;
&lt;li&gt;Write tests and check regression&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hope you found this post useful. Discussion is more than welcome. Please share your thoughts in comments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introducing CloudCV bootstrap</title>
      <link>/post/introducing-cloudcv-bootstrap/</link>
      <pubDate>Tue, 14 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/introducing-cloudcv-bootstrap/</guid>
      <description>

&lt;p&gt;Here&amp;rsquo;s an open-source ready to use bootstrap project written in Node.js that lets
you to quickly build a REST service to host your image processing and computer vision code
in a cloud environment.
Please welcome: &lt;a href=&#34;https://github.com/CloudCV/cloudcv-bootstrap&#34;&gt;cloudcv-bootstrap&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;cloudcv-bootstrap.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I made this project aside of CloudCV to keep it simple but functionaly. It is self-contained
Node.js project that helps you to get quick results on building and deploying your first
server-based image processing service.&lt;/p&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Ready to use. No need to download extra dependencies. Just run &lt;code&gt;npm install&lt;/code&gt; and that&amp;rsquo;s all.&lt;/li&gt;
&lt;li&gt;Built-in REST-API support. As a bonus, a Swagger 2.0 specification file comes too. You can use it as a template to build client SDKs.&lt;/li&gt;
&lt;li&gt;Shipped with OpenCV 3.0.0&lt;/li&gt;
&lt;li&gt;Interopability between C++ and Node.js code&lt;/li&gt;
&lt;li&gt;Covered with unit tests&lt;/li&gt;
&lt;li&gt;Logging support&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With cloudcv-bootstrap you can quickly wrap your C++ code into web-service using simple and
clear syntax:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;app.post(&#39;/api/v1/image/analyze/dominantColors/&#39;, function (req, res) {
    cv.analyzeImage(req.files.image.buffer, function(error, result) {
        res.setHeader(&amp;quot;Content-Type&amp;quot;, &amp;quot;application/json&amp;quot;);
        res.write(JSON.stringify(MapAnalyzeResult(result)));
        res.end();
    });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Error handling and logging here omited for the sake of simplicity, but this is full-functional snippet.
It accepts uploaded image using POST request and transfers image data to C++ backend.
&lt;a href=&#34;https://github.com/CloudCV/cloudcv-bootstrap&#34;&gt;cloudcv-bootstrap&lt;/a&gt; fully follows Node.js programming paradigm and schedule C++ code on libuv thread pool and leave main thread free for requests processing.&lt;/p&gt;

&lt;h2 id=&#34;quick-start&#34;&gt;Quick start&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/CloudCV/cloudcv-bootstrap.git
npm install
npm start &amp;amp;
curl localhost:3000/api/v1/image/analyze/dominantColors?image=https%3A%2F%2Fraw.githubusercontent.com%2FCloudCV%2Fcloudcv-bootstrap%2Fmaster%2Ftest%2Fdata%2Fopencv-logo.jpg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Produces:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;{
    &amp;quot;aspect&amp;quot;:
    {
        &amp;quot;width&amp;quot;:599,
        &amp;quot;height&amp;quot;:555
    },
    &amp;quot;size&amp;quot;:
    {
        &amp;quot;width&amp;quot;:0,
        &amp;quot;height&amp;quot;:0
    },
    &amp;quot;dominantColors&amp;quot;:
    [
        {&amp;quot;color&amp;quot;:[252,252,252],&amp;quot;totalPixels&amp;quot;:201655,&amp;quot;interclassVariance&amp;quot;:7.83907795204613e-37,&amp;quot;error&amp;quot;:0},
        {&amp;quot;color&amp;quot;:[252,0,0],&amp;quot;totalPixels&amp;quot;:43612,&amp;quot;interclassVariance&amp;quot;:7.83907795204613e-37,&amp;quot;error&amp;quot;:0},
        {&amp;quot;color&amp;quot;:[0,0,252],&amp;quot;totalPixels&amp;quot;:43591,&amp;quot;interclassVariance&amp;quot;:7.83907795204613e-37,&amp;quot;error&amp;quot;:0},
        {&amp;quot;color&amp;quot;:[0,252,0],&amp;quot;totalPixels&amp;quot;:43587,&amp;quot;interclassVariance&amp;quot;:7.83907795204613e-37,&amp;quot;error&amp;quot;:0}
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Congratulations, you&amp;rsquo;ve just computed dominant colors of the OpenCV logo image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CloudCV/cloudcv-bootstrap/master/test/data/opencv-logo.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;extending-with-your-code&#34;&gt;Extending with your code&lt;/h2&gt;

&lt;p&gt;This module uses node-gyp build system. It produces Node C++ addon and require you to do minimal changes into this module:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Have C++ code you want to host&lt;/li&gt;
&lt;li&gt;Write module binding&lt;/li&gt;
&lt;li&gt;Register it&lt;/li&gt;
&lt;li&gt;Write unit tests&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;rsquo;s go step by step using camera calibration as example. For quick results we won&amp;rsquo;t reinvent the wheel and use code from OpenCV samples. I will just refactor it slightly. Here&amp;rsquo;s our public interface of calibration algorithm:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;enum PatternType {
    CHESSBOARD = 0,
    CIRCLES_GRID = 1,
    ACIRCLES_GRID = 2
};

class CameraCalibrationAlgorithm
{
public:
    typedef std::vector&amp;lt;cv::Point3f&amp;gt;               VectorOf3DPoints;
    typedef std::vector&amp;lt;cv::Point2f&amp;gt;               VectorOf2DPoints;
    typedef std::vector&amp;lt;std::vector&amp;lt;cv::Point3f&amp;gt; &amp;gt; VectorOfVectorOf3DPoints;
    typedef std::vector&amp;lt;std::vector&amp;lt;cv::Point2f&amp;gt; &amp;gt; VectorOfVectorOf2DPoints;
    typedef std::vector&amp;lt;cv::Mat&amp;gt;                   VectorOfMat;

    CameraCalibrationAlgorithm(cv::Size patternSize, PatternType type);

    bool detectCorners(const cv::Mat&amp;amp; frame, VectorOf2DPoints&amp;amp; corners2d) const;

    bool calibrateCamera(
        const VectorOfVectorOf2DPoints&amp;amp; gridCorners,
        const cv::Size imageSize,
        cv::Mat&amp;amp; cameraMatrix,
        cv::Mat&amp;amp; distCoeffs
    ) const;

protected:

    // .. plenty of helper methods

private:
    cv::Size                 m_patternSize;
    PatternType              m_pattern;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We want to wrap it into V8 code. First, we need to register corresponding function that we will expose to JS:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void RegisterModule(Handle&amp;lt;Object&amp;gt; target)
{
    // ...

    NODE_SET_METHOD(target, &amp;quot;calibrationPatternDetect&amp;quot;, calibrationPatternDetect);
    NODE_SET_METHOD(target, &amp;quot;calibrateCamera&amp;quot;,          calibrateCamera);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Implementation of &lt;code&gt;calibrationPatternDetect&lt;/code&gt; and &lt;code&gt;calibrateCamera&lt;/code&gt; needs to parse input arguments, schedule a task to thread pool and invoke a user-passed callback on completition.
Marshalling between C++ and V8 is tricky.
Fortunately, NaN module does a great help on data marshalling.
To simplity developer&amp;rsquo;s life even more &lt;a href=&#34;https://github.com/CloudCV/cloudcv-bootstrap&#34;&gt;cloudcv-bootstrap&lt;/a&gt; offers complex data marshalling and argument checking:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;NAN_METHOD(calibrationPatternDetect)
{
    TRACE_FUNCTION;
    NanEscapableScope();

    Local&amp;lt;Object&amp;gt;   imageBuffer;
    Local&amp;lt;Function&amp;gt; callback;
    cv::Size        patternSize;
    PatternType     pattern;
    std::string     error;

    if (NanCheck(args)
        .Error(&amp;amp;error)
        .ArgumentsCount(4)
        .Argument(0).IsBuffer().Bind(imageBuffer)
        .Argument(1).Bind(patternSize)
        .Argument(2).StringEnum&amp;lt;PatternType&amp;gt;({ 
            { &amp;quot;CHESSBOARD&amp;quot;,     PatternType::CHESSBOARD }, 
            { &amp;quot;CIRCLES_GRID&amp;quot;,   PatternType::CIRCLES_GRID }, 
            { &amp;quot;ACIRCLES_GRID&amp;quot;,  PatternType::ACIRCLES_GRID } }).Bind(pattern)
        .Argument(3).IsFunction().Bind(callback))
    {
        LOG_TRACE_MESSAGE(&amp;quot;Parsed function arguments&amp;quot;);
        NanCallback *nanCallback = new NanCallback(callback);
        NanAsyncQueueWorker(new DetectPatternTask(
            CreateImageSource(imageBuffer), 
            patternSize, 
            pattern, 
            nanCallback));
    }
    else if (!error.empty())
    {
        LOG_TRACE_MESSAGE(error);
        NanThrowTypeError(error.c_str());
    }

    NanReturnUndefined();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may read about NanCheck in separate post: &lt;a href=&#34;http://computer-vision-talks.com/articles/how-to-convert-args-from-js-to-cpp&#34;&gt;NanCheck&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;data-marshalling&#34;&gt;Data marshalling&lt;/h2&gt;

&lt;p&gt;Natively, marshaller supports:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C++ plain types&lt;/strong&gt;:
 - char, unsigned char
 - short, unsighed short
 - int, unsigned int
 - long, unsigned long
 - float, double
 - T[N]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;STL types&lt;/strong&gt;:
 - std::array&lt;T,N&gt;
 - std::pair&lt;A,B&gt;
 - std::vector&lt;T&gt;
 - std::map&lt;K,V&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;OpenCV types&lt;/strong&gt;:
 - cv::Point2i, cv::Point2f, cv::Point2d
 - cv::Size&lt;em&gt;&lt;int&gt;, cv::Size&lt;/em&gt;&lt;float&gt;, cv::Size_&lt;double&gt;
 - cv::Mat&lt;/p&gt;

&lt;p&gt;Data marshalling of user-defined structures implemented in similar to boost::serialization fashion:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;struct CalibrationResult
{
    cv::Mat  m_distCoeffs;
    cv::Mat  m_cameraMatrix;
    bool     m_calibrationSuccess;

    template &amp;lt;typename Archive&amp;gt;
    void serialize(Archive&amp;amp; ar)
    {
        ar &amp;amp; serialization::make_nvp(&amp;quot;calibrationSuccess&amp;quot;, m_calibrationSuccess);

        if (Archive::is_loading::value || m_calibrationSuccess)
        {
            ar &amp;amp; serialization::make_nvp(&amp;quot;cameraMatrix&amp;quot;,m_cameraMatrix);
            ar &amp;amp; serialization::make_nvp(&amp;quot;distCoeffs&amp;quot;,  m_distCoeffs);
        }
    }
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;User-defined types will be marshalled to regular V8 object containing fields serialized within &lt;code&gt;serialize()&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;To marshal C++ object to V8 object:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;CalibrationResult cpp_result = ...;
auto v8_result = marshal(cpp_result);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To marshal from V8 object to C++ object:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;v8::Local&amp;lt;v8::Value&amp;gt; v8_result = ...;
auto cpp_result = marshal&amp;lt;CalibrationResult&amp;gt;(v8_result);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;roadmap&#34;&gt;Roadmap&lt;/h2&gt;

&lt;p&gt;This is very beta version of cloudcv-bootstrap and it&amp;rsquo;s codebase about to change.
Please keep in mind that and feel free to ask for help in &lt;a href=&#34;https://twitter.com/cvtalks&#34;&gt;twitter&lt;/a&gt; or on &lt;a href=&#34;https://github.com/CloudCV/cloudcv-bootstrap/issues&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;According to plan:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Add Dockerfile to run this code in a container environment&lt;/li&gt;
&lt;li&gt;Write more documentation on data marshalling&lt;/li&gt;
&lt;li&gt;Implement easier REST API mapping and arguments checking&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>How to debug node.js addons in Visual Studio</title>
      <link>/post/how-to-debug-nodejs-addons-in-visual-studio/</link>
      <pubDate>Tue, 17 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/how-to-debug-nodejs-addons-in-visual-studio/</guid>
      <description>

&lt;p&gt;While working on &lt;a href=&#34;https://cloudcv.io&#34;&gt;CloudCV&lt;/a&gt; I encountered problems in node.js addon written in native code. For CloudCV I use node.js with C++ Addon to separate high-performance algorithms (C++) from high-level networking API which node provides.&lt;/p&gt;

&lt;p&gt;In this tutorial I&amp;rsquo;m going to reveal best practices on debugging C++ Addons for Node.js (0.12) using Visual Studio 2013.&lt;/p&gt;

&lt;div class=&#34;embed-responsive embed-responsive-4by3&#34;&gt;
  &lt;iframe class=&#34;embed-responsive-item&#34; src=&#34;https://www.youtube.com/embed/eqhv42jVN6s&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Continue reading if you want to read in details why this works.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This article is valid for Node.js version 0.12. It should also works fine for further releases, however few things may change. I will try to keep this post up to date. Please, feel free to drop a comment for this article or write me on &lt;a href=&#34;https://twitter.com/cvtalks&#34;&gt;@cvtalks&lt;/a&gt; if you have troubles following this tutorial.&lt;/p&gt;

&lt;h2 id=&#34;before-we-start&#34;&gt;Before we start&lt;/h2&gt;

&lt;p&gt;You will need a Visual Studio 2013 (&lt;a href=&#34;http://go.microsoft.com/?linkid=9832256&#34;&gt;Express edition&lt;/a&gt; should be enough). Also, please come and grab &lt;a href=&#34;http://nodejs.org/dist/v0.12.0/node-v0.12.0.tar.gz&#34;&gt;Node.js source&lt;/a&gt;] code. We will use it later to build Debug configuration of Node.js. I assume you
already have production (Release) version of Node and NPM as we will need &lt;a href=&#34;https://github.com/TooTallNate/node-gyp&#34;&gt;node-gyp&lt;/a&gt; to generate and build C++ Addon project. So please ensure you&amp;rsquo;ve installed them before going to next step:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install -g node-gyp
npm install -g nan
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;build-debug-configuration-of-node-js&#34;&gt;Build Debug configuration of node.js&lt;/h2&gt;

&lt;p&gt;This is very straighforward step.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download &lt;a href=&#34;http://nodejs.org/dist/v0.12.0/node-v0.12.0.tar.gz&#34;&gt;Node.js source&lt;/a&gt;] code.&lt;/li&gt;
&lt;li&gt;Extract it somewhere to your filesystem. For demonstration, I assume it will be in &lt;code&gt;c:\Develop\node-v0.12.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Navigate to &lt;code&gt;c:\Develop\node-v0.12.0&lt;/code&gt; and run &lt;code&gt;vcbuild.bat debug nosign x64&lt;/code&gt;. This batch script will build Debug configuration of node.js for 64-bit architecture (if you&amp;rsquo;re on 32-bit platform omit this flag). A &lt;code&gt;nosign&lt;/code&gt; flag tells to skip executable signing which is ok since we&amp;rsquo;re not going to distribute it, and finally &lt;code&gt;debug&lt;/code&gt; forces compiler to generate debug symbols for node executable.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;build-node-debug-step-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If everything goes fine, you should see the following output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;c:\Develop\node-v0.12.0&amp;gt;vcbuild.bat debug nosign x64
ctrpp not found in WinSDK path--using pre-gen files from tools/msvs/genfiles.
creating  icu_config.gypi

...

creating  config.gypi
creating  config.mk
Project files generated.

...

debugger-agent.vcxproj -&amp;gt; c:\Develop\node-v0.12.0\Debug\lib\debugger-agent.lib
v8_nosnapshot.vcxproj -&amp;gt; ..\..\..\..\build\Debug\lib\v8_nosnapshot.lib
openssl-cli.vcxproj -&amp;gt; c:\Develop\node-v0.12.0\Debug\\openssl-cli.exe
mksnapshot.vcxproj -&amp;gt; ..\..\..\..\build\Debug\\mksnapshot.exe
v8_snapshot.vcxproj -&amp;gt; ..\..\..\..\build\Debug\lib\v8_snapshot.lib
node.vcxproj -&amp;gt; c:\Develop\node-v0.12.0\Debug\\node.exe
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;build-node-debug-step-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you have problems on this step, please refer to &lt;a href=&#34;https://github.com/joyent/node/wiki/installation#building-on-windows&#34;&gt;building node.js&lt;/a&gt; official documentation.&lt;/p&gt;

&lt;h2 id=&#34;building-debug-configuration-of-c-addon&#34;&gt;Building Debug configuration of C++ Addon&lt;/h2&gt;

&lt;p&gt;A C++ Addon for nodejs is nothing but ordinary DLL. Therefore Visual Studio can load it and let you do step-by-step debugging inside node.js app.
Let&amp;rsquo;s start with simple scenario. I will use &lt;a href=&#34;https://github.com/rvagg/nan/tree/master/examples/async_pi_estimate&#34;&gt;Pi estimation&lt;/a&gt; example from &lt;a href=&#34;https://github.com/rvagg/nan&#34;&gt;NaN&lt;/a&gt; project.&lt;/p&gt;

&lt;p&gt;Your C++ Addon code can look as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;/*********************************************************************
 * NAN - Native Abstractions for Node.js
 *
 * Copyright (c) 2015 NAN contributors
 *
 * MIT License &amp;lt;https://github.com/rvagg/nan/blob/master/LICENSE.md&amp;gt;
 ********************************************************************/

var addon = require(&#39;./build/Release/addon&#39;);
var calculations = process.argv[2] || 100000000;

function printResult(type, pi, ms) {
  console.log(type, &#39;method:&#39;)
  console.log(&#39;\tπ ≈ &#39; + pi
        + &#39; (&#39; + Math.abs(pi - Math.PI) + &#39; away from actual)&#39;)
    console.log(&#39;\tTook &#39; + ms + &#39;ms&#39;);
    console.log()
}

function runSync () {
  var start = Date.now();
  // Estimate() will execute in the current thread,
  // the next line won&#39;t return until it is finished
    var result = addon.calculateSync(calculations);
  printResult(&#39;Sync&#39;, result, Date.now() - start)
}

function runAsync () {
  // how many batches should we split the work in to?
    var batches = process.argv[3] || 16;
    var ended = 0;
    var total = 0;
    var start = Date.now();

    function done (err, result) {
        total += result;

    // have all the batches finished executing?
        if (++ended == batches) {
            printResult(&#39;Async&#39;, total / batches, Date.now() - start)
        }
    }

  // for each batch of work, request an async Estimate() for
  // a portion of the total number of calculations
    for (var i = 0; i &amp;lt; batches; i++) {
        addon.calculateAsync(calculations / batches, done);
    }
}

runSync()
runAsync()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And you also should have gyp file which tells node-gyp how to build your addon:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-gyp&#34;&gt;{
  &amp;quot;targets&amp;quot;: [
    {
      &amp;quot;target_name&amp;quot;: &amp;quot;addon&amp;quot;,
      &amp;quot;sources&amp;quot;: [
        &amp;quot;addon.cc&amp;quot;,
        &amp;quot;pi_est.cc&amp;quot;,
        &amp;quot;sync.cc&amp;quot;,
        &amp;quot;async.cc&amp;quot;
      ],
      &amp;quot;include_dirs&amp;quot;: [&amp;quot;&amp;lt;!(node -e \&amp;quot;require(&#39;nan&#39;)\&amp;quot;)&amp;quot;]
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming you&amp;rsquo;re in root C++ addon directory, you can now generate a solution for Visual Studio to build your addon:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;node-gyp configure rebuild --nodedir=&amp;quot;c:\Develop\node-v0.12.0&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;build-addon-step-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;What is really important here, is &lt;code&gt;--nodedir=&amp;quot;c:\Develop\node-v0.12.0&amp;quot;&lt;/code&gt; flag, which indicates to link against node in specified
folder rather than system wide available.&lt;/p&gt;

&lt;p class=&#34;bg-info lead&#34;&gt;
This is very important to match Debug node with Debug C++ Addon, otherwise you will have 
linker issues caused by inconsistent CRT&#39;s.
&lt;/p&gt;

&lt;p&gt;After node-gyp finishes, a .node file (this is .dll, don&amp;rsquo;t be misleaded by .node extension) will be generated in /build/Debug/ folder.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;build-addon-step-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now it&amp;rsquo;s time to write a small node.js script that utilizes our addon:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nodejs&#34;&gt;var addon = require(&#39;./build/Release/addon&#39;);

function printResult(type, pi, ms) {
  console.log(type, &#39;method:&#39;)
  console.log(&#39;\tπ ≈ &#39; + pi
        + &#39; (&#39; + Math.abs(pi - Math.PI) + &#39; away from actual)&#39;)
    console.log(&#39;\tTook &#39; + ms + &#39;ms&#39;);
    console.log()
}

function runAsync () {

    var start = Date.now();

    function done (err, result) {
        console.log(&#39;\tπ ≈ &#39; + pi + &#39; (&#39; + Math.abs(pi - Math.PI) + &#39; away from actual)&#39;)
        console.log(&#39;\tTook &#39; + ms + &#39;ms&#39;);
        console.log()

        printResult(&#39;Async&#39;, total / batches, Date.now() - start)
    }

    addon.calculateAsync(1024, done);
}

runAsync()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may wonder now, how to debug it. Patience, we&amp;rsquo;re almost there.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;generated-project.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Navigate to &lt;code&gt;build/&lt;/code&gt; directory of your C++ Addon and
open solution file.&lt;/li&gt;
&lt;li&gt;Ensure that you have active Debug configuration.&lt;/li&gt;
&lt;li&gt;Navigate to project properties and open Debugging tab there.&lt;/li&gt;
&lt;li&gt;Modify command name to &amp;ldquo;c:\Develop\node-v0.12.0\Node.exe&amp;rdquo; (Change this path if you extracted node somewhere else)&lt;/li&gt;
&lt;li&gt;Set a command argument to full name of your node.js script.&lt;/li&gt;
&lt;li&gt;Change working directory to a place where your script is. This step important when you have complex nodej.js application with
dependencies.&lt;/li&gt;
&lt;li&gt;Set breakpoint somewhere in your C++ Addon code.&lt;/li&gt;
&lt;li&gt;Now hit &amp;lsquo;Debug&amp;rsquo; (F5) and enjoy!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;vsproject-settings.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;how-it-works&#34;&gt;How it works&lt;/h2&gt;

&lt;p&gt;When building Debug builds, Visual Studio trades speed for easy debugging. This means not only slower code, but it also preserves debug
symbols (a table of function addresses/names/file locations). When you start a debugger on process, VS attachs to it and tries to load
symbols for this binary and all dynamically loaded libraries it uses.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;vsproject-debugging.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As a consequence, debug symbols for C++ addon is being loaded which allows you to see program execution location in your IDE,
do step-by-step debugging and change/rebuild/debug as usual.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I hope you find this post useful. From my experience debugging node.js &amp;lt;-&amp;gt; C++ interop can be nasty. Personally I follow this
scenario for debugging CloudCV C++ backend. This saves a lot of time and nerves. Unleash your node.js with C++ and happy debugging!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hacking OpenCV for fun and profit</title>
      <link>/post/hacking-opencv-for-fun-and-profit/</link>
      <pubDate>Thu, 25 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/hacking-opencv-for-fun-and-profit/</guid>
      <description>

&lt;div class=&#34;featured-image&#34;&gt;
![](logo.png)
&lt;/div&gt;

&lt;p&gt;This post convers very specific but important topic about writing memory-efficient code.
I will show you how to collect and analyze memory allocations that happens in OpenCV.&lt;/p&gt;

&lt;p&gt;When it comes to writing efficient code we usually care about CPU-efficiency. However there are
many times, when memory-efficiency is more important. A limited amount of RAM is not so rare as
one can think. On iOS and Android there are a strict memory usage restrictions, and of your app
uses more memory than allowed your app can get killed by the system. Embedded hardware systems
used in IoT, Raspberri Pi and others also have very limited amount of RAM. So you should be very
careful when porting code from desktop with gigabytes of memory to mobile platform.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&#34;why-memory-matters&#34;&gt;Why memory matters&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start with a small example of &lt;a href=&#34;https://en.wikipedia.org/wiki/Unsharp_masking&#34;&gt;unsharp masking&lt;/a&gt; to illustrate the problem:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;cv::Mat source, gray, grayFloat, blurred, unsharped;
source = cv::imread(&amp;quot;/some/path/to/image.png&amp;quot;);
cv::cvtColor(m, gray, cv::COLOR_BGR2GRAY)
gray.convertTo(grayFloat, CV_32F, 1.0 / 255.0);
cv::GaussianBlur(grayFloat, blurred, cv::Size(3,3));
unsharped = blurred * 1.5f - grayFloat * 0.5f;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How much additional memory required for this piece of code to work? Let&amp;rsquo;s count:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;source = cv::imread(&amp;quot;/some/path/to/image.png&amp;quot;);         // N*M*3 bytes
cv::cvtColor(m, gray, cv::COLOR_BGR2GRAY)               // N*M bytes
gray.convertTo(grayFloat, CV_32F, 1.0 / 255.0);         // N*M*4
cv::GaussianBlur(grayFloat, blurred, cv::Size(3,3));    // N*M*4
unsharped = blurred * 1.5f - grayFloat * 0.5f;          // N*M*4   (in the best case)
                                                        // N*M*4*3 (in the worst case)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For NxM sized image we require at least sixteen (16!) times more memory in temporary variables.
Funny, but the code looks harmless for the first look. Now you can think what will happen if you
put this snippet into iOS application. Pictures made with iPhone 5S are about 3200x2400 pixels.
On such image this code will allocate 128Mb of memory!&lt;/p&gt;

&lt;p&gt;I will leave it for you as a home-work to figure out a solution to minimize memory usage by this function.
In this post I want to demonstrate how to measure memory usage in OpenCV automatically. Complex projects are
harder to analyze like example above, so you definitely not going to re-calculate it after each change.&lt;/p&gt;

&lt;h2 id=&#34;hacking-opencv&#34;&gt;Hacking OpenCV&lt;/h2&gt;

&lt;p&gt;Typically, when it comes to memory allocation tracing, we usually overload &lt;code&gt;new operator&lt;/code&gt; to intercept all
allocations. In OpenCV it becomes even easier. There is a &lt;code&gt;cv::fastMalloc&lt;/code&gt; function that is a memory allocator for
all OpenCV project. This means every &lt;code&gt;cv::Mat&lt;/code&gt; allocation use it. So our goal is to change cv::fastMalloc to &amp;lsquo;save&amp;rsquo;
allocations somewhere where we can access it in runtime.&lt;/p&gt;

&lt;p&gt;To serve this purpose, I will write a helper class to store allocations/deallocations data. For analysys purposes,
it records peak memory usage, allocations count, current memory usage and number of live objects:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;// modules\core\include\opencv2\core.hpp
struct CV_EXPORTS MemorySnapshot
{
    //! Total amount of allocated memory.
    size_t allocatedMemory;

    //! Maximum amount of allocated memory for the whole time.
    size_t peakMemoryUsage;

    //! Maximum amount of allocated memory since last snapshot.
    size_t peakMemoryUsageSinceLastSnapshot;
    
    //! Number of memory allocations count for the whole program running time.
    size_t allocationsCount;

    //! Number of memory deallocations for the whole program running time.
    size_t deallocationsCount;
    
    //! Number of allocated objects that are still live (e.g not deallocated).
    size_t liveObjects;
};

CV_EXPORTS MemorySnapshot memorySnapshot();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;An implementation of &lt;code&gt;MemoryManager&lt;/code&gt; is very trivial and can be extended to collect statistics on individual allocations.
However for my needs this implementation was more than enough:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;// modules\core\alloc.cpp
class MemoryManager
{
public:
    //! Key - pointer to allocated memory, Value - it&#39;s size
    typedef std::map&amp;lt;void*, size_t&amp;gt;     AllocationTable;
    typedef std::lock_guard&amp;lt;std::mutex&amp;gt; LockType;

    void recordAlloc(void* ptr, size_t size)
    {
        LockType guard(mAllocMutex);
        mAllocatedMemory.insert(std::make_pair(ptr, size));

        mCurrentMemoryUsage += size;
        mPeakMemoryUsage = std::max(mPeakMemoryUsage, mCurrentMemoryUsage);
        mPeakMemoryUsageSinceLastSnapshot = std::max(mPeakMemoryUsageSinceLastSnapshot, mCurrentMemoryUsage);
        mAllocationsCount++;
    }

    void recordFree(void* ptr)
    {
        LockType guard(mAllocMutex);

        auto block = mAllocatedMemory.find(ptr);
        CV_Assert(block != mAllocatedMemory.end());
    
        mCurrentMemoryUsage -= block-&amp;gt;second;
        mDeallocationsCount++;
        mAllocatedMemory.erase(block);
    }

   

    static MemoryManager&amp;amp; Instance()
    {
        std::call_once(mInitFlag, []() {
            if (mInstance == nullptr)
            {
                mInstance = new MemoryManager();
            }
        });

        return *mInstance;
    }

    MemorySnapshot makeSnapshot()
    {
        LockType guard(mAllocMutex);
        
        MemorySnapshot snapshot;
        snapshot.peakMemoryUsage = mPeakMemoryUsage;
        snapshot.peakMemoryUsageSinceLastSnapshot = mPeakMemoryUsageSinceLastSnapshot;
        snapshot.allocatedMemory = mCurrentMemoryUsage;
        snapshot.allocationsCount = mAllocationsCount;
        snapshot.deallocationsCount = mDeallocationsCount;
        snapshot.liveObjects = mAllocationsCount - mDeallocationsCount;
        
        mPeakMemoryUsageSinceLastSnapshot = 0;

        return std::move(snapshot);
    }
private:

    MemoryManager()
        : mCurrentMemoryUsage(0)
        , mPeakMemoryUsage(0)
        , mPeakMemoryUsageSinceLastSnapshot(0)
        , mAllocationsCount(0)
        , mDeallocationsCount(0)
    {
    }

private:
    std::mutex      mAllocMutex;
    AllocationTable mAllocatedMemory;

    size_t          mCurrentMemoryUsage;
    size_t          mPeakMemoryUsage;
    size_t          mPeakMemoryUsageSinceLastSnapshot;

    size_t          mAllocationsCount;
    size_t          mDeallocationsCount;

    static std::once_flag  mInitFlag;
    static MemoryManager * mInstance;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now it&amp;rsquo;s time to inject our MemoryManager into &lt;code&gt;cv::fastAlloc&lt;/code&gt; / &lt;code&gt;cv::fastFree&lt;/code&gt; functions and create new function &lt;code&gt;cv::memorySnaphot&lt;/code&gt; to retrieve memory snapshots:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;// modules\core\alloc.cpp
void* fastMalloc( size_t size )
{
    uchar* udata = (uchar*)malloc(size + sizeof(void*) + CV_MALLOC_ALIGN);

    if(!udata)
        return OutOfMemoryError(size);

    MemoryManager::Instance().recordAlloc(udata, size);
    uchar** adata = alignPtr((uchar**)udata + 1, CV_MALLOC_ALIGN);
    adata[-1] = udata;
    return adata;
}

void fastFree(void* ptr)
{
    if(ptr)
    {
        uchar* udata = ((uchar**)ptr)[-1];
        CV_DbgAssert(udata &amp;lt; (uchar*)ptr &amp;amp;&amp;amp;
               ((uchar*)ptr - udata) &amp;lt;= (ptrdiff_t)(sizeof(void*)+CV_MALLOC_ALIGN));

        MemoryManager::Instance().recordFree(udata);
        free(udata);
    }
}

MemorySnapshot memorySnapshot()
{
    return std::move(MemoryManager::Instance().makeSnapshot());
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s all with hacking OpenCV. One last step is to rebuild OpenCV with any C++11 conformant compiler and we can use memory manager for analyzing our code.&lt;/p&gt;

&lt;h2 id=&#34;fun-profit&#34;&gt;Fun &amp;amp; Profit&lt;/h2&gt;

&lt;p&gt;Back to first example, we want to measure memory usage after each step. Here&amp;rsquo;s how C++ macros can help us:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;cv::Mat source, gray, grayFloat, blurred, unsharped;
MEASURE_MEMORY(m = cv::imread(&amp;quot;/some/path/to/image.png&amp;quot;));
MEASURE_MEMORY(cv::cvtColor(m, gray, cv::COLOR_BGR2GRAY));
MEASURE_MEMORY(gray.convertTo(grayFloat, CV_32F, 1.0 / 255.0));
MEASURE_MEMORY(cv::GaussianBlur(grayFloat, blurred, cv::Size(5, 5), 5));
MEASURE_MEMORY(unsharped = blurred * 1.5f - grayFloat * 0.5f);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where &lt;code&gt;MEASURE_MEMORY&lt;/code&gt; is a helper macro defined as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#define MEASURE_MEMORY(x) { size_t memOnStart = cv::memorySnapshot().allocatedMemory; x; \
                            size_t memOnEnd = cv::memorySnapshot().allocatedMemory;      \
                            std::cout &amp;lt;&amp;lt; #x &amp;lt;&amp;lt; &amp;quot;\t&amp;quot; &amp;lt;&amp;lt; memOnStart &amp;lt;&amp;lt; &amp;quot;/&amp;quot; &amp;lt;&amp;lt; memOnEnd &amp;lt;&amp;lt; std::endl; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Example output is the following (The image was 3200x2400):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;m = cv::imread(&amp;quot;/some/path/to/image.png&amp;quot;)               //      743/ 23971559 +23Mb
cv::cvtColor(m, gray, cv::COLOR_BGR2GRAY)               // 23971559/ 31961831 +7.6Mb
gray.convertTo(grayFloat, CV_32F, 1.0 / 255.0)          // 31961831/ 63922919 +30Mb
cv::GaussianBlur(grayFloat, blurred, cv::Size(5, 5), 5) // 63922919/ 95884007 +30Mb
unsharped = blurred * 1.5f - grayFloat * 0.5f           // 95884007/127845095 +30Mb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, numbers are really close to predicted. This way you can measure memory usage, peak usage,
number of allocations in your program easily, spot memory-related issues and fix them before they appear on
customer&amp;rsquo;s hardware.&lt;/p&gt;

&lt;h2 id=&#34;source-code&#34;&gt;Source code&lt;/h2&gt;

&lt;p&gt;The modified OpenCV source code can be found here: &lt;a href=&#34;https://github.com/BloodAxe/opencv/tree/memory-snapshots&#34;&gt;https://github.com/BloodAxe/opencv/tree/memory-snapshots&lt;/a&gt;. Don&amp;rsquo;t forget to enable it by building OpenCV with ENABLE_MEMORY_SNAPSHOTS=YES option.&lt;/p&gt;

&lt;p&gt;I have sent pull-request to OpenCV team, so there is a chance it will be included into official OpenCV. Let&amp;rsquo;s keep fingers crossed.&lt;/p&gt;

&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;

&lt;p&gt;The code is available for use any commercial and non-commercial purposes, but please keep a credit by providing a
link to my website &lt;a href=&#34;http://computer-vision-talks.com&#34;&gt;computer-vision-talks.com&lt;/a&gt; and email &lt;a href=&#34;ekhvedchenya@gmail.com&#34;&gt;ekhvedchenya@gmail.com&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tile-based image processing</title>
      <link>/post/tile-based-image-processing/</link>
      <pubDate>Thu, 04 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/tile-based-image-processing/</guid>
      <description>

&lt;p&gt;How would you design an algorithm to process 40Mpx image? 100Mpx? What about gigapixel-sized panorams? Obviously, it should differs from those that are intended for 640x480 images. Here I want to present you implementation of the very simple but powerful approach called &amp;ldquo;Tile-based image processing&amp;rdquo;. I will show you how to make this using OpenCV.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;tiles.png&#34; alt=&#34;Tile based image processing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34; /&gt;&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s define a few restrictions in order to simplify our implementation. In this tutorial I will consider a &amp;lsquo;pass-through&amp;rsquo; pipeline - when we apply some function to input image and give an output image of the same size as an output.&lt;/p&gt;

&lt;p&gt;It is possible to extend this approach to work with many input images, but for the sake of simplicity I&amp;rsquo;ll omit this for now.&lt;/p&gt;

&lt;p&gt;Consider a following algorithm:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Take a source image for RGB color space.&lt;/li&gt;
&lt;li&gt;Convert in to grayscale color space (unsigned byte).&lt;/li&gt;
&lt;li&gt;Compute Sobel derivatives (signed short).&lt;/li&gt;
&lt;li&gt;Take a Dx, Dy for each pixel and compute it&amp;rsquo;s magnitude and orientation.&lt;/li&gt;
&lt;li&gt;Leave only those, which magnitude is larger than threshold.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Using OpenCV it could look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;cv::Mat source = cv::imread(&amp;quot;input.jpg&amp;quot;);
cv::Mat grayscale, dx, dy;
cv::cvtColor(source, grayscale);
cv::Sobel(grayscale, dx, 1, 0);
cv::Sobel(grayscale, dy, 0, 1);
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;problems-with-straighforward-implementation&#34;&gt;Problems with straighforward implementation&lt;/h2&gt;

&lt;p&gt;This routine require &lt;code&gt;N + 2 * N * sizeof(signed short)&lt;/code&gt; bytes of additional memory for straightforward implementation, where N is number of pixels in source image. Large number of intermediate buffers can cause memory issues for memory restricted devices (mobile phones, embedded systems).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;On iOS, in particular, your app might get terminated by iOS watchdog for high peak RAM usage, despite the fact you use this memory only for a temp buffers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Second issue with large amount of buffers is cache-misses. Large buffers are likely to sit near each other, therefore cache performance will be low and algorithm performance will suffer.&lt;/p&gt;

&lt;p&gt;To adress those two issues, I suggest to divide input image into &amp;ldquo;Tiles&amp;rdquo; - regions of the original image of equal size, let&amp;rsquo;s say 64x64. The processing function remains the same, but we reuse all temporary buffers and process only 64x64 pixels at one time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;algorithm.png&#34; alt=&#34;Tile based image processing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say we&amp;rsquo;re processing &lt;code&gt;1280x720&lt;/code&gt; frame, using regular approach, the total amount of
additional memory is &lt;strong&gt;4.6 megabytes&lt;/strong&gt; (&lt;code&gt;4608000&lt;/code&gt; bytes). With tile-based approach, we need only &lt;strong&gt;20 kilobytes&lt;/strong&gt; (&lt;code&gt;20480&lt;/code&gt; bytes). 20K are likely to fit entirely in L2 cache and therefore give a significant performance boost.&lt;/p&gt;

&lt;h2 id=&#34;tile-based-implementation&#34;&gt;Tile-based implementation&lt;/h2&gt;

&lt;p&gt;To implement tile-based implementation, we iterate over the image, copy tiles from source image to our local source tile, process it and write to corresponding area in the
destination image.&lt;/p&gt;

&lt;p&gt;A pseudo-code for this routine is follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;int TileSize, typename Algorithm&amp;gt;
void process(const cv::Mat&amp;amp; sourceImage, cv::Mat&amp;amp; resultImage, Algorithm algorithm) const
{
    assert(!resultImage.empty());
    assert(sourceImage.rows == resultImage.rows);
    assert(sourceImage.cols == resultImage.cols);

    const int rows = (sourceImage.rows / TileSize) + (sourceImage.rows % TileSize ? 1 : 0);
    const int cols = (sourceImage.cols / TileSize) + (sourceImage.cols % TileSize ? 1 : 0);

    cv::Mat tileInput, tileOutput;

    for (int rowTile = 0; rowTile &amp;lt; rows; rowTile++)
    {
        for (int colTile = 0; colTile &amp;lt; cols; colTile++)
        {
            copyTileFromSource(sourceImage, tileInput, rowTile, colTile);
            algorithm(tileInput, tileOutput);
            copyTileToResultImage(tileOutput, resultImage, rowTile, colTile);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I hope it&amp;rsquo;s clear to understand what is happening in code above. The &lt;code&gt;Algorithm&lt;/code&gt; here represents some algorithm that we want to run on our tiles. There are two functions &lt;code&gt;copyTileFromSource&lt;/code&gt; and &lt;code&gt;copyTileToResultImage&lt;/code&gt; that will be covered a bit later.&lt;/p&gt;

&lt;h2 id=&#34;dealing-with-out-of-tile-reads&#34;&gt;Dealing with out-of-tile reads&lt;/h2&gt;

&lt;p&gt;You may ask yourself - what should we do with border pixels? Sobel operator use neighbor pixels around each pixel. When we construct a tile shouldn&amp;rsquo;t we take this into account? Sure we are. So that&amp;rsquo;s why there is a padding parameter that controls amount of additional pixels that are added to top, left, bottom and right of the tile in order to make functions that require additional pixels work correct.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;tile_with_paddings.png&#34; alt=&#34;Tile with paddings&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Padding makes tile overlap each other, but we pay this price for good cache locality.&lt;/p&gt;

&lt;p&gt;I will use a slightly modified version of code from above:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;struct TiledAlgorithm
{
    TiledAlgorithm(int tileSize, int padding, int borderType)
        : mTileSize(tileSize)
        , mPadding(padding)
        , mBorderType(borderType)
    {
    }

    void process(const cv::Mat&amp;amp; sourceImage, cv::Mat&amp;amp; resultImage) const
    {
        assert(!resultImage.empty());
        assert(sourceImage.rows == resultImage.rows);
        assert(sourceImage.cols == resultImage.cols);

        int rows = (sourceImage.rows / mTileSize) + (sourceImage.rows % mTileSize ? 1 : 0);
        int cols = (sourceImage.cols / mTileSize) + (sourceImage.cols % mTileSize ? 1 : 0);

        cv::Mat tileInput, tileOutput;

        for (int rowTile = 0; rowTile &amp;lt; rows; rowTile++)
        {
            for (int colTile = 0; colTile &amp;lt; cols; colTile++)
            {
                cv::Rect srcTile(colTile * mTileSize - mPadding, 
                                 rowTile * mTileSize - mPadding, 
                                 mTileSize + 2 * mPadding, 
                                 mTileSize + 2 * mPadding);

                cv::Rect dstTile(colTile * mTileSize,            
                                 rowTile * mTileSize, 
                                 mTileSize, 
                                 mTileSize);

                copySourceTile(sourceImage, tileInput, srcTile);
                processTileImpl(tileInput, tileOutput);
                copyTileToResultImage(tileOutput, resultImage, dstTile);
            }
        }
    }

protected:
    virtual void processTileImpl(const cv::Mat&amp;amp; srcTile, cv::Mat&amp;amp; dstTile) const = 0;
    
    void copySourceTile(const cv::Mat&amp;amp; src, cv::Mat&amp;amp; srcTile, cv::Rect &amp;amp;tile) const;
    void copyTileToResultImage(const cv::Mat&amp;amp; tileImage, cv::Mat&amp;amp; resultImage, cv::Rect resultRoi);

};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;processing_with_paddings.png&#34; alt=&#34;Processing with paddings&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To fill a tile with source image we should check whether tile is close to image border. In this case OpenCV will come to help with cv::copyMakeBorder function that helps us to fill the missing pixels with given border fill method. If tile including paddings are entirely in the image boundary, it&amp;rsquo;s enough to just copy image region to a tile:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void copySourceTile(const cv::Mat&amp;amp; src, cv::Mat&amp;amp; srcTile, cv::Rect &amp;amp;tile)
{
    auto tl = tile.tl();
    auto br = tile.br();

    cv::Point tloffset, broffset;

    //Take care of border cases
    if (tile.x &amp;lt; 0)
    {
        tloffset.x = -tile.x;
        tile.x = 0;
    }

    if (tile.y &amp;lt; 0)
    {
        tloffset.y = -tile.y;
        tile.y = 0;
    }

    if (br.x &amp;gt;= src.cols)
    {
        broffset.x = br.x - src.cols + 1;
        tile.width -= broffset.x;
    }

    if (br.y &amp;gt;= src.rows)
    {
        broffset.y = br.y - src.rows + 1;
        tile.height -= broffset.y;
    }

    // If any of the tile sides exceed source image boundary we must use copyMakeBorder to make proper paddings for this side
    if (tloffset.x &amp;gt; 0 || tloffset.y &amp;gt; 0 || broffset.x &amp;gt; 0 || broffset.y &amp;gt; 0)
    {
        cv::Rect paddedTile(tile.tl(), tile.br());
        assert(paddedTile.x &amp;gt;= 0);
        assert(paddedTile.y &amp;gt;= 0);
        assert(paddedTile.br().x &amp;lt; src.cols);
        assert(paddedTile.br().y &amp;lt; src.rows);

        cv::copyMakeBorder(src(paddedTile), srcTile, tloffset.y, broffset.y, tloffset.x, broffset.x, mBorderType);
    }
    else
    {
        // Entire tile (with paddings lies inside image and it&#39;s safe to just take a region:
        src(tile).copyTo(srcTile);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For non-zero padding case we add additional pixels to source tile, therefore it has effective width and height of &lt;code&gt;TileSize + Padding + Padding&lt;/code&gt;, but after processing we write only central segment of the tile of size &lt;code&gt;TileSize x TileSize&lt;/code&gt; to destination image. In case of Sobel, we need a padding of &lt;code&gt;1&lt;/code&gt;, because Sobel uses 3x3 kernel by default.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void copyTileToResultImage(const cv::Mat&amp;amp; tileImage, cv::Mat&amp;amp; resultImage, cv::Rect resultRoi)
{
    cv::Rect srcTile(mPadding, mPadding, mTileSize, mTileSize);

    auto br = resultRoi.br();

    if (br.x &amp;gt;= resultImage.cols)
    {
        resultRoi.width -= br.x - resultImage.cols;
        srcTile.width -= br.x - resultImage.cols;
    }

    if (br.y &amp;gt;= resultImage.rows)
    {
        resultRoi.height -= br.y - resultImage.rows;
        srcTile.height -= br.y - resultImage.rows;
    }

    cv::Mat tileView = tileImage(srcTile);
    cv::Mat dstView = resultImage(resultRoi);

    assert(tileView.rows == dstView.rows);
    assert(tileView.cols == dstView.cols);

    tileView.copyTo(dstView);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;application&#34;&gt;Application&lt;/h2&gt;

&lt;p&gt;This approach can be used when you need to guarantee low-memory footprint of your algorithm or you want to use data locality without changing a lot in your code. In this
case I suggest to pre-allocate data buffers as a continuous block of memory:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;// Our algorithm need three intermediate buffers: a,b,c that
// we want to store close to each other
class MyAlgorithm : public TiledAlgorithm
{
public:
    MyAlgorithm(int tileSize, int padding)
    {
        int size = tileSize + padding * 2;

        // Allocate all buffer as continuous array
        mBuffer.create(size * 3, size, CV_8UC1);
            
        // Create views to sub-regions of mBuffer
        a = mBuffer.rowRange(0,      size);
        b = mBuffer.rowRange(size,   2*size);
        c = mBuffer.rowRange(2*size, 3*size);
    }

private:
    cv::Mat mBuffer;

    cv::Mat a, b c;
}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Did you know, that JPEG-2000 coded use tile-based encoding and it allows this codec to retrieve (decode) an arbitrary region of the image? Also, tiles are widely used in aerial photography to stich images.&lt;/p&gt;

&lt;p&gt;I hope you find this post interesting. Pleas let me know on which topics you would like to see in my blog. Feel free to drop a ping on &lt;a href=&#34;https://twitter.com/cvtalks&#34;&gt;@cvtalks&lt;/a&gt; or leave a comment. Thanks!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Image processing in your browser - Unit Test automation</title>
      <link>/post/image-processing-in-your-browser-unit-test-automation/</link>
      <pubDate>Fri, 31 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/image-processing-in-your-browser-unit-test-automation/</guid>
      <description>&lt;p&gt;JavaScript. Do you like debug JavaScript code? I hate it. Literally.
What what if you have to? In this post I&amp;rsquo;m going to show you how to
simplify your life by automating unit testing of the JavaScript code
for the browser.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;monkeys.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To get things more interesting - let&amp;rsquo;s automate unit-testing of the
image processing library called &lt;a href=&#34;http://inspirit.github.io/jsfeat/&#34;&gt;JSFeat&lt;/a&gt;. JSFeat provides a
JavaScript implementation of the basic image processing operations
that let you to process images in your browser and build sophisticated
algorithms. &lt;strong&gt;It&amp;rsquo;s like OpenCV for web-browser&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/BloodAxe/jsfeat&#34;&gt;source code&lt;/a&gt; for this tutorial is available on my Github page: &lt;a href=&#34;https://github.com/BloodAxe/jsfeat&#34;&gt;https://github.com/BloodAxe/jsfeat&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Typically, when we test C++ software, we end up with test framework of your
choise and test runner. It can be continuous integration server like Jenkins
in your company or public Travic-CI service for open-source project.&lt;/p&gt;

&lt;p&gt;With browser JavaScript things gets more complicated. I&amp;rsquo;m not talking about
JS unit-test frameworks - &lt;a href=&#34;https://github.com/mochajs/mocha&#34;&gt;mocha&lt;/a&gt; is more than enough. I&amp;rsquo;m talking about
browser testing itself. Basically you have to open a webpage in your browser to
invoke a test cases. Manually. Ew!&lt;/p&gt;

&lt;p&gt;Moreover, due to browser sandbox, you are not allowed to access canvas data for
local files. In practice it means that code like showed below &lt;strong&gt;won&amp;rsquo;t work if you
open a HTML page as a local file&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;var img = new Image();
img.src = &#39;dummy.jpg&#39;;
img.onload = function() {
    // This will never happen
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will have to setup a local HTTP server to serve these tests pages to get this
works. One simple way to do it by using python: &lt;code&gt;python -m SimpleHTTPServer 8000&lt;/code&gt; will
do the job. However it&amp;rsquo;s only a partial solution.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s imagine that you have a lot of these tests. Would you open each page
individually, watch how they run and wait to analyze their results? No doubts,
you can do it, but this is not automated testing in any way.&lt;/p&gt;

&lt;p&gt;To recap, here are a list of problems that exists in browser JS testing:
 - A test framework
 - Have to open test pages in a browser
 - Hard to automate and collect results
 - Need local webserver&lt;/p&gt;

&lt;p&gt;Since I&amp;rsquo;m used to Mocha, i will use it. However it&amp;rsquo;s not obligatory and you can chose any other test framework you like. But as you will see later, with Mocha it&amp;rsquo;s really simple. ith mocha you can write your scripts like showed below. This is a real test I wrote as an example for JSFeat:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;// Source test/test_grayscale.js
&#39;use strict&#39;;

describe(&#39;jsfeat&#39;, function(){

  describe(&#39;imgproc&#39;, function(){

    it(&#39;grayscale&#39;, function(done) {

      var img = new Image();
      img.src = &#39;lena.png&#39;;
      img.onload = function() {

        var width = img.width;
        var height = img.height;

        var canvas = document.createElement(&#39;canvas&#39;);
        var context2d = canvas.getContext(&#39;2d&#39;);

        context2d.drawImage(img, 0, 0, width, height);
        var image_data = context2d.getImageData(0, 0, width, height);
 
        var gray_img = new jsfeat.matrix_t(width, height, jsfeat.U8_t | jsfeat.C1_t);
        var code = jsfeat.COLOR_RGBA2GRAY;

        jsfeat.imgproc.grayscale(image_data.data, width, height, gray_img, code);
        done();
      };

    });

  });
   
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is a really powerful tool in Nodejs world called [Grunt][grunt] that we will use
to automate tasks like JavaScript static code checking, minification and testing.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install -g grunt-cli
npm install grunt --save-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A headless browser is a full-featured browser engine without graphical interface. It was designed to simulate a real browser including DOM and JavaScript. The most important headless browser is &lt;a href=&#34;http://phantomjs.org/&#34;&gt;PhantomJS&lt;/a&gt;. I found that it works like a charm for this task. With phantomjs we can run arbitrary HTML page inside and execute JavaScript code. This tool let&amp;rsquo;s us to get rid of the manual tabs openning.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;// script.js:
// Simple Javascript example

console.log(&#39;Loading a web page&#39;);
var page = require(&#39;webpage&#39;).create();
var url = &#39;http://www.phantomjs.org/&#39;;
page.open(url, function (status) {
  //Page is loaded!
  phantom.exit();
});

phantomjs script.js
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;PhantomJS and Mocha are already connected together in a single grunt task called &lt;a href=&#34;https://github.com/jdcataldo/grunt-mocha-phantomjs&#34;&gt;grunt-mocha-phantomjs&lt;/a&gt;. And we use &lt;a href=&#34;https://github.com/gruntjs/grunt-contrib-connect&#34;&gt;grunt-contrib-connect&lt;/a&gt; to host a local webserver.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install grunt-mocha-phantomjs --save-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This extension does exactly what we need: it starts a local webserver, open page in phantomjs and run JS test cases.
With help of it, we are able to run all our tests using simple command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;grunt test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here&amp;rsquo;s an example output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Running &amp;quot;concat:jsfeat&amp;quot; (concat) task
File build/jsfeat.js created.

Running &amp;quot;uglify:build&amp;quot; (uglify) task
&amp;gt;&amp;gt; 1 file created.

Running &amp;quot;connect:server&amp;quot; (connect) task
Started connect web server on http://0.0.0.0:8000

Running &amp;quot;mocha_phantomjs:all&amp;quot; (mocha_phantomjs) task


  jsfeat
    imgproc
      ✓ grayscale 


  1 passing (41ms)


Done, without errors.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Within this setup you are now able to automate testing of JavaScript code that require interaction with HTML5 Canvas features. This way I test the code that I write for browser image processing. I hope you enjoyed this post and I&amp;rsquo;m looking forward to see your questions and mentions in a comments!&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/BloodAxe/jsfeat&#34;&gt;source code&lt;/a&gt; for this tutorial is available on my Github page: &lt;a href=&#34;https://github.com/BloodAxe/jsfeat&#34;&gt;https://github.com/BloodAxe/jsfeat&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computer Vision Digest - September 2014</title>
      <link>/post/computer-vision-digest-september-2014/</link>
      <pubDate>Wed, 01 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/computer-vision-digest-september-2014/</guid>
      <description>

&lt;p&gt;Third &lt;a href=&#34;/tags/digest.html&#34;&gt;computer vision digest&lt;/a&gt;. Your monthly portion of news in computer vision for September 2014.&lt;/p&gt;

&lt;p&gt;In this issue:
 - &lt;a href=&#34;#1&#34;&gt;Real-time face 3D model reconstruction&lt;/a&gt;
 - &lt;a href=&#34;#1&#34;&gt;Image color correction and contrast enhancement&lt;/a&gt;
 - &lt;a href=&#34;#3&#34;&gt;Robust Optimization Techniques in Computer Vision&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Previous issues:
 - &lt;a href=&#34;/articles/2014-05-computer-vision-digest/&#34;&gt;Computer Vision Digest (May 2014)&lt;/a&gt;
 - &lt;a href=&#34;/articles/2014-06-computer-vision-digest/&#34;&gt;Computer Vision Digest (June 2014)&lt;/a&gt;
 - &lt;a href=&#34;/articles/computer-vision-digest-august-2014/&#34;&gt;Computer Vision Digest (August 2014)&lt;/a&gt;&lt;/p&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
Feel free to leave your suggestions on interesting materials in post comments 
or via Twitter by mentioning [@cvtalks](https://twitter.com/cvtalks). 
Best links will be included into next digest!
&lt;/div&gt;

&lt;p&gt;&lt;span class=&#34;more clearfix&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;1&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;real-time-face-3d-model-reconstruction&#34;&gt;Real-time face 3D model reconstruction&lt;/h2&gt;

&lt;p&gt;Researchers from the University of Washington prepared interesting presentation for the European Conference on Computer Vision (ECCV-2014). It is a real-time &lt;a href=&#34;http://grail.cs.washington.edu/projects/totalmoving/&#34;&gt;3D face reconstruction from the video&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;15a03e37860948f9b2c4925b3c311c45.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Using the video from YouTube, the program automatically builds highly detailed face 3D-model for each video frame.&lt;/p&gt;

&lt;p&gt;This is a very impressive result, given the complexity of the problem, because the facial expressions of the human face is very complex. For emotion recognition, it is important to see the exact position of the eyes, bending eyebrows, wrinkles. The smallest error in reconstructed 3D-model is highly noticeable.&lt;/p&gt;

&lt;iframe width=&#34;800&#34; height=&#34;600&#34; src=&#34;//www.youtube.com/embed/C1iLVAUiC7s&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;The vast majority of other programs for face 3D-tracking uses blend shapes method, when the shape of the object changes, &amp;ldquo;flowing&amp;rdquo; from one state to another. The method of smooth deformations has lack of the small details that are so important for the perception of faces. The authors of the new algorithm abandoned this approach.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;2639b10e54684d11b684d3257c8f400c.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;On the other hand, although the frame-independent reconstruction create a &amp;ldquo;separate&amp;rdquo; 3D mode from each frame, when you play on the &lt;sup&gt;30&lt;/sup&gt;&amp;frasl;&lt;sub&gt;60&lt;/sub&gt; frames per second, the result should be more realistic than in the case of a smooth modification.&lt;/p&gt;

&lt;p&gt;And more. Unlike other technologies, it does not require human involvement in a test of a movie. Instead, a large archive of his photographs in different lighting conditions and poses, this method use video footrage that is tracked with optical flow (3D optical flow). Author research say that in our time for each person collected a large archive of photographs that can be used to reconstruct it&amp;rsquo;s face.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;28b4d17c1c65450faa683cc1afeddd89.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Source: &lt;a href=&#34;http://habrahabr.ru/post/237827/&#34;&gt;http://habrahabr.ru/post/237827/&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;image-color-correction-and-contrast-enhancement&#34;&gt;Image color correction and contrast enhancement&lt;/h2&gt;

&lt;p&gt;A friend of mine shared a link to slideshare to the exhaustive research and analysis of color correction and contrast enchancement algorithms. How many of these have you worked with? I was impressed on how much algorithms has been developed so far. Just watch these slides, I bet - you&amp;rsquo;ll find new algorithms you&amp;rsquo;ve never heard about. Cheers to Yu Huang for collecting them for us!&lt;/p&gt;

&lt;p&gt;&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/28271598?rel=0&#34; width=&#34;597&#34; height=&#34;486&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;https://www.slideshare.net/yuhuang/image-color-correction-contrast-adjustment&#34; title=&#34;Image color correction and contrast enhancement&#34; target=&#34;_blank&#34;&gt;Image color correction and contrast enhancement&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;http://www.slideshare.net/yuhuang&#34; target=&#34;_blank&#34;&gt;Yu Huang&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;3&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;robust-optimization-techniques-in-computer-vision&#34;&gt;Robust Optimization Techniques in Computer Vision&lt;/h2&gt;

&lt;div class=&#34;alert alert-danger&#34; role=&#34;alert&#34;&gt;
    &lt;strong&gt;Math warning.&lt;/strong&gt; Do not read this section unless you understand what damping function is and what is LevMar.
&lt;/div&gt;

&lt;p&gt;I&amp;rsquo;ve found nice slides from the ECCV 2014 workshop on non-linear optimization problems that happen in computer vision.&lt;/p&gt;

&lt;p&gt;Course description&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&#34;pull-left&#34;&gt;
![nl](nl.png)
&lt;/div&gt;

&lt;p&gt;Many important problems in computer vision, such as structure from motion and image registration, involve model estimation in presence of a significant number of outliers. Due to the outliers, simple estimation techniques such as least squares perform very poorly. To deal with this issue, vision researchers have come up with a number of techniques that are robust to outliers, such as Hough transform and RANSAC (random sample consensus). These methods will be analyzed with respect to statistical modeling, worst-case and average exectution times and how to choose the balance between the number of outliers and the number of inliers. Apart from these classical techniques we will also describe recent advances in robust model estimation. This includes sampling based techniques with guaranteed optimality for low-dimensional problems and optimization of semi-robust norms for high-dimensional problems. We will see how to solve low-dimensional estimation problems with over 99% outliers in a few seconds, as well as how to detect outliers in structure from motion problems with thousands of variables.
Topics&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www2.maths.lth.se/matematiklth/personal/fredrik/Session1.pdf&#34;&gt;Session 1&lt;/a&gt;: Statistical models of robust regression. Introduction, motivations and applications. Relation to robust statistics. Occasional vs. frequent large-scale measurement noise (outliers). Low- vs. high-dimensional model estimation. Optimal vs. approximate methods. Multiple model fitting. Computational complexity.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www2.maths.lth.se/matematiklth/personal/fredrik/Session2.pdf&#34;&gt;Session 2&lt;/a&gt;: Robust estimation with low-dimensional models. Hough transform. M-estimators. RANSAC and its variants. Branch and bound methods. Optimal methods. Fast approximate methods. Applications: Feature-based registration, multiple-view geometry, image-based localization.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www2.maths.lth.se/matematiklth/personal/fredrik/Session3.pdf&#34;&gt;Session 3&lt;/a&gt;: Robust estimation with high-dimensional models. Robust norms and convex optimization. L_infinity-norm optimization with outliers. L_1-norm optimization on manifolds. Applications: Multiple-view geometry, large-scale structure-from-motion and subspace estimation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can read it here: &lt;a href=&#34;http://www2.maths.lth.se/matematiklth/personal/fredrik/eccv2014_tutorial.html&#34;&gt;http://www2.maths.lth.se/matematiklth/personal/fredrik/eccv2014_tutorial.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Argument checking for native addons for Node.js. Do it right!</title>
      <link>/post/how-to-convert-args-from-js-to-cpp/</link>
      <pubDate>Thu, 11 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/how-to-convert-args-from-js-to-cpp/</guid>
      <description>

&lt;div class=&#34;featured-image&#34;&gt;
![NanCheck](logo.jpg)
&lt;/div&gt;

&lt;p&gt;During development of &lt;a href=&#34;https://cloudcv.io&#34;&gt;CloudCV&lt;/a&gt; I came to the problem on converting &lt;code&gt;v8::Arguments&lt;/code&gt; to
native C++ data types in my Node.js native module. If you are new to C++ and Node.js, I suggest you to read how to write C++ modules for Node.js and connecting OpenCV and Node.js first.&lt;/p&gt;

&lt;p&gt;Mapping V8 data types to native C++ equivalents is trivial, but somewhat wordy. One should take the
argument at given index, check whether it is defined, then check it&amp;rsquo;s type and finally cast to C++ type.
This works fine while you have function that receive two or three arguments of trivial type (That can be mapped directly to built-in C++ types). What about strings? Arrays? Complex types like objects or function callback?
You code will grow like and became hard-to-maintain pasta-code some day.&lt;/p&gt;

&lt;p&gt;In this post I present my approach on solving this problem with a laconic way on describing what do you expect as input arguments.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34;&gt;&lt;/span&gt;
&lt;div class=&#34;clearfix&#34;&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;To illustrate the difference between imperative approach I included source code for calibrationPatternDetect method that expose function to detect calibration pattern on a single image to Node.js code. As you may see below, there are a lot of &lt;em&gt;if&lt;/em&gt; conditions, magic numbers and no type checking for a half of arguments. But even without it, this function occupy 50 lines of code.
What even worse, 90% of this code is going to be the same for other functions. The main purpose of code of any &lt;code&gt;NAN_METHOD&lt;/code&gt; implementation - to marshal data in such a way it can be used by C++ code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;NAN_METHOD(calibrationPatternDetect)
{
    NanScope();

    if (args.Length() != 5)
    {
        return NanThrowError(&amp;quot;Invalid number of arguments&amp;quot;);  
    }

    if (!args[0]-&amp;gt;IsObject())
    {
        return NanThrowTypeError(&amp;quot;First argument should be a Buffer&amp;quot;);      
    }

    // 0 - image
    // 1 - width
    // 2 - height
    // 3 - pattern
    // 4 - callback

    int w  = args[1]-&amp;gt;Uint32Value();
    int h  = args[2]-&amp;gt;Uint32Value();
    int pt = args[3]-&amp;gt;Uint32Value();
    PatternType pattern;

    switch (pt)
    {
        case 0:
            pattern = CHESSBOARD;
            break;

        case 1:
            pattern = CIRCLES_GRID;
            break;

        case 2:
            pattern = ASYMMETRIC_CIRCLES_GRID;
            break;    

        default:
            return NanThrowError(&amp;quot;Unsupported pattern type. Only 0 (CHESSBOARD), 1 (CIRCLES_GRID) or 2 (ASYMMETRIC_CIRCLES_GRID) are supported.&amp;quot;);
    };

    if (!args[4]-&amp;gt;IsFunction())
    {
        return NanThrowTypeError(&amp;quot;Last argument must be a function.&amp;quot;);
    }

    // The task holds our custom status information for this asynchronous call,
    // like the callback function we want to call when returning to the main
    // thread and the status information.

    NanCallback *callback = new NanCallback(args[4].As&amp;lt;Function&amp;gt;());
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So the goal is to add more syntax sugar for argument checking.
Basically, it should provide a convenient way to check number and type of arguments passed.
For &lt;a href=&#34;https://cloudcv.io&#34;&gt;CloudCV&lt;/a&gt; project I&amp;rsquo;ve ended with a declarative approach because I found it fit my needs very much
and makes argument checking self-explanatory. Here is how new implementation of &lt;code&gt;calibrationPatternDetect&lt;/code&gt; looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;NAN_METHOD(calibrationPatternDetect)
{
    NanScope();

    Local&amp;lt;Object&amp;gt;   imageBuffer;
    Local&amp;lt;Function&amp;gt; callback;
    cv::Size        patternSize;
    PatternType     pattern;

    try
    {
        if (NanCheck(args).ArgumentsCount(5)
            .Argument(0).IsBuffer().Bind(imageBuffer)
            .Argument(1).Bind(patternSize.width)
            .Argument(2).Bind(patternSize.height)
            .Argument(3).StringEnum&amp;lt;PatternType&amp;gt;({ 
                { &amp;quot;CHESSBOARD&amp;quot;,     PatternType::CHESSBOARD }, 
                { &amp;quot;CIRCLES_GRID&amp;quot;,   PatternType::CIRCLES_GRID }, 
                { &amp;quot;ACIRCLES_GRID&amp;quot;,  PatternType::ACIRCLES_GRID } }).Bind(pattern)
            .Argument(4).IsFunction().Bind(callback))
        {
            NanCallback *nanCallback = new NanCallback(callback);
            NanAsyncQueueWorker(new DetectPatternTask(imageBuffer, patternSize, pattern, nanCallback));
            NanReturnValue(NanTrue());
        }

        NanReturnValue(NanFalse());
    }
    catch (ArgumentMismatchException exc)
    {
        return NanThrowTypeError(exc.what());
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I hope you agree that second version is much more easy to read. Fluent architecture allows to write predicates in a chain, which actually is very similar to the way we thing. All predicate has self-telling names made from verb and a noun. So let me give you a brief overview what &lt;code&gt;NanCheck&lt;/code&gt; is capable of.&lt;/p&gt;

&lt;h2 id=&#34;fluent-api&#34;&gt;Fluent API&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Method_chaining&#34;&gt;Method chaining&lt;/a&gt; (aka Fluent API) makes it very easy to build final predicate for argument checking via consecutive checks.
Each next step will be made &lt;strong&gt;if and only if&lt;/strong&gt; all previous predicates were successful.
In case of error, predicate will throw an &lt;code&gt;ArgumentMismatchException&lt;/code&gt; exception that will terminate all further checks. &lt;code&gt;NanCheck(args)&lt;/code&gt; can be evaluated to &lt;code&gt;bool&lt;/code&gt; which makes it possible to use NanCheck in a condition statement:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;if (NanCheck(args). ...) {
    // This code will be executed if argument parsing
    // will be successful        
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;type-checking&#34;&gt;Type checking&lt;/h2&gt;

&lt;p&gt;To check particular argument at given index, &lt;code&gt;NanCheckArguments&lt;/code&gt; provide a &lt;code&gt;Argument(index)&lt;/code&gt; function. This function lets you to build a sub-predicate for given argument and bind it&amp;rsquo;s value with particular local variable:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    if (NanCheck(args).Argument(0).IsBuffer()) {
        // This code will be executed if argument parsing
        // will be successful        
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Currently, &lt;code&gt;NanCheck&lt;/code&gt; support type checking of the following built-in V8 types:
1. &lt;code&gt;v8::Function&lt;/code&gt;
2. &lt;code&gt;v8::Object&lt;/code&gt;
3. &lt;code&gt;v8::String&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In addition, it offers &lt;code&gt;NotNull&lt;/code&gt; predicate to ensure argument is not null or empty.
The list of predicates will grow for sure. New functions to check whether argument is &lt;code&gt;v8::Array&lt;/code&gt;, &lt;code&gt;v8::Number&lt;/code&gt;, &lt;code&gt;v8::Integer&lt;/code&gt;, &lt;code&gt;v8::Boolean&lt;/code&gt; will be added in a next updates.&lt;/p&gt;

&lt;h2 id=&#34;binding&#34;&gt;Binding&lt;/h2&gt;

&lt;p&gt;After type checking, it&amp;rsquo;s necessary to complete sub-predicate construction by &lt;em&gt;binding&lt;/em&gt; argument to a local variable. Binding is a assignment of the argument (with data marshaling, if it&amp;rsquo;s necessary) to a variable that will be used later;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    Local&amp;lt;Object&amp;gt;   imageBuffer;
    if (NanCheck(args).Argument(0).IsBuffer().Bind(imageBuffer)) {
        // This code will be executed if argument parsing
        // will be successful        
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NanCheck support transparent binding to all v8 types (Number, String, Function, Object, Array, etc.), native C++ and OpenCV types (via &lt;a href=&#34;https://github.com/BloodAxe/CloudCVBackend/blob/master/src/framework/marshal/opencv.cpp&#34;&gt;marshaling system&lt;/a&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    cv::Size        patternSize;
    if (NanCheck(args).Argument(1).IsObject().Bind(patternSize)) {
        // This code will be executed if argument parsing
        // will be successful        
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is a special case of string arguments called &lt;code&gt;StringEnum&lt;/code&gt; - that is, a string argument, which can be one of a priory defined values. It introduced to support *&lt;em&gt;C++ enum&lt;/em&gt; types and pass them
as string constants. &lt;code&gt;StringEnum&lt;/code&gt; predicate allow to parse string value and map to C++ enum type:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    PatternType     pattern;
    if (NanCheck(args)
        .Argument(3).StringEnum&amp;lt;PatternType&amp;gt;({ 
            { &amp;quot;CHESSBOARD&amp;quot;,     PatternType::CHESSBOARD }, 
            { &amp;quot;CIRCLES_GRID&amp;quot;,   PatternType::CIRCLES_GRID }, 
            { &amp;quot;ACIRCLES_GRID&amp;quot;,  PatternType::ACIRCLES_GRID } }).Bind(pattern)) {
        // This code will be executed if argument parsing
        // will be successful        
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;implementation-highlights&#34;&gt;Implementation highlights&lt;/h2&gt;

&lt;p&gt;Thanks to C++11, it&amp;rsquo;s really easy to construct predicate chain using lambda functions. Basically predicate chain is nothing but a recursive anonymous function of the following form:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    auto initFn = [innerPredicate, outerPredicate](const v8::Arguments&amp;amp; args) {
        return innerPredicate(args) &amp;amp;&amp;amp; outerPredicate(args);
    };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To illustrate an idea of building predicate chain, let&amp;rsquo;s take a look on ArgumentsCount implementation:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    NanCheckArguments&amp;amp; NanCheckArguments::ArgumentsCount(int count)
    {
        return AddAndClause([count](const v8::Arguments&amp;amp; args) 
        { 
            if (args.Length() != count)
                throw ArgumentMismatchException(args.Length(), count); 

            return true;
        });
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we construct outer predicate which compare number of arguments to expected value
and throw an exception if it does not match.&lt;/p&gt;

&lt;p&gt;With a help of &lt;code&gt;std::initializer_list&lt;/code&gt; it became really simple to declare string enum with minimal syntax overhead:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    class NanMethodArgBinding
    {
    public:
    ...
        template &amp;lt;typename T&amp;gt;
        NanArgStringEnum&amp;lt;T&amp;gt; 
        StringEnum(std::initializer_list&amp;lt; std::pair&amp;lt;const char*, T&amp;gt; &amp;gt; possibleValues);
    ...
    };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we&amp;rsquo;re able to call this function with arbitrary number of elements for this enum using
&lt;code&gt;std::initializer_list&lt;/code&gt; syntax:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    { 
        { &amp;quot;CHESSBOARD&amp;quot;,     PatternType::CHESSBOARD }, 
        { &amp;quot;CIRCLES_GRID&amp;quot;,   PatternType::CIRCLES_GRID }, 
        { &amp;quot;ACIRCLES_GRID&amp;quot;,  PatternType::ACIRCLES_GRID }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/BloodAxe/CloudCVBackend/blob/master/src/framework/NanCheck.hpp&#34;&gt;&lt;strong&gt;NanCheck&lt;/strong&gt;&lt;/a&gt; helped me to reduce amount of code required to check arguments passed to &lt;a href=&#34;https://cloudcv.io&#34;&gt;CloudCV&lt;/a&gt; back-end. There are many cool ideas that I&amp;rsquo;ll probably add as soon as there will be necessity to have them in my library:
- Strongly typed objects (Objects with required fields)
- Optional parameters with default values
- Automatic type inference based on &lt;code&gt;Bind&amp;lt;T&amp;gt;(...)&lt;/code&gt; type.
- Support of multiple types per argument (Parameter can be either of type A or B)&lt;/p&gt;

&lt;p&gt;Please leave your comments on this post. I&amp;rsquo;ve spent many hours on figuring out how to implement data marshaling and type checking in V8 and Node.js, so please help information to
spread out - share and re-tweet this post. Cheers!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computer Vision Digest - August 2014</title>
      <link>/post/computer-vision-digest-august-2014/</link>
      <pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/computer-vision-digest-august-2014/</guid>
      <description>

&lt;p&gt;Third &lt;a href=&#34;/tags/digest.html&#34;&gt;computer vision digest&lt;/a&gt;. Your monthly portion of news in computer vision for August 2014.&lt;/p&gt;

&lt;p&gt;In this issue:
 - &lt;a href=&#34;#1&#34;&gt;Free Photo Editing Software Lets You Manipulate Objects in 3D&lt;/a&gt;
 - &lt;a href=&#34;#2&#34;&gt;Real-Time Digital Makeup with Projection Mapping&lt;/a&gt;
 - &lt;a href=&#34;#3&#34;&gt;Video stabilization through 3D scene recovery&lt;/a&gt;
 - &lt;a href=&#34;#4&#34;&gt;Using OpenCV, Python and Template Matching to play “Where’s Waldo?”&lt;/a&gt;
 - &lt;a href=&#34;#5&#34;&gt;OpenCV 3.0 alpha is out&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Previous issues:
 - &lt;a href=&#34;/articles/2014-05-computer-vision-digest/&#34;&gt;Computer Vision Digest (May 2014)&lt;/a&gt;
 - &lt;a href=&#34;/articles/2014-06-computer-vision-digest/&#34;&gt;Computer Vision Digest (June 2014)&lt;/a&gt;&lt;/p&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
Feel free to leave your suggestions on interesting materials in post comments 
or via Twitter by mentioning [@cvtalks](https://twitter.com/cvtalks). 
Best links will be included into next digest!
&lt;/div&gt;

&lt;p&gt;&lt;span class=&#34;more clearfix&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;1&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;free-photo-editing-software-lets-you-manipulate-objects-in-3d&#34;&gt;Free Photo Editing Software Lets You Manipulate Objects in 3D&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://i.kinja-img.com/gawker-media/image/upload/s--CtQ_vCt9--/c_fit,fl_progressive,q_80,w_636/sbuewdyltzbjdmfjvgof.gif&#34; alt=&#34;Free Photo Editing Software Lets You Manipulate Objects in 3D&#34; /&gt;&lt;/p&gt;

&lt;p&gt;How much Photoshop magic can you make with 2D photo? This software can do more! SIGGRAPH 2014 showed us a method that enables users to perform the full range of 3D manipulations, including scaling, rotation, translation, and nonrigid deformations, to an object in a photograph. Despite the fact it has limitations to use of stock 3D models set that are available for manipulation, it is great demonstration on how 2D and 3D can be combined together to bring image manipulation for the next level. I think Adobe is already buying these guys (and one girl).&lt;/p&gt;

&lt;p&gt;The cool news, there are free demo, source code and publication paper that you can read:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~om3d/sourcecodeversions.html&#34;&gt;Source code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~om3d/agreement.html&#34;&gt;OS X (Mavericks) Executable Code and Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~om3d/papers/SIGGRAPH2014.pdf&#34;&gt;Publication paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;real-time-digital-makeup-with-projection-mapping&#34;&gt;Real-Time Digital Makeup with Projection Mapping&lt;/h2&gt;

&lt;p&gt;This is how state of the art technologies comes to real life. Well studied algorithms and a bit of tech = amazing results. Projection mapping in conjunction with real-time face tracking made possible a virtual make-up! No more words. Watch this:&lt;/p&gt;

&lt;iframe src=&#34;//player.vimeo.com/video/103425574?byline=0&amp;amp;portrait=0&amp;amp;badge=0&amp;amp;color=cfcaca&#34; width=&#34;853&#34; height=&#34;480&#34; frameborder=&#34;0&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt; 

&lt;p&gt;A true beauty of augmented reality. Girls, you don&amp;rsquo;t need to do a make-up for virtual date anymore :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;So how did they made it?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I assume this involves real-time frame tracker that outputs a face 3D model which is 99% tuned for particular person via offline training (Google: Active appearance model).
Have you noticed white dots on her face? These are special markers that are used to &amp;ldquo;wire&amp;rdquo; face 3D model to real one.&lt;/p&gt;

&lt;p&gt;And then they take virtual makeup (A texture that mapped onto 3D face model) and deform it to match tracked model. A projector then maps virtual makeup onto actor.&lt;/p&gt;

&lt;p&gt;Well done, OMOTE. This was great demonstration!&lt;/p&gt;

&lt;p&gt;Source: &lt;a href=&#34;http://www.augmentedrealitytrends.com/augmented-reality/projection-mapping.html&#34;&gt;Real-Time Digital Makeup with Projection Mapping&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;3&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;hyperlapse-video-stabilization-through-3d-scene-recovery&#34;&gt;Hyperlapse video stabilization through 3D scene recovery&lt;/h2&gt;

&lt;p&gt;This is not about Instagram :)&lt;/p&gt;

&lt;p&gt;Microsoft Research showed more sophisticated video stabilization algorithms for making Hyperlapse video from the raw footage made with ordinary handheld camera.&lt;/p&gt;

&lt;iframe width=&#34;853&#34; height=&#34;480&#34; src=&#34;//www.youtube.com/embed/SOpwHaQnRSY?rel=0&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Developers claim that their result impossible to achieve using alternative ways of stabilization. The method is based on the reconstruction of the 3D-scene, and then algorithm optimize &amp;ldquo;movement&amp;rdquo; of the camera along the route in order to avoid vibration, and combines the images pixel by pixel to smooth video sequence.&lt;/p&gt;

&lt;p&gt;For example, the figure below shows this route (in black) and an optimized route, which is generated by the application to render the video (red).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;microsoft-hyperlapse-path-planning.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The result is a so-called hyperlapse-video (named by analogy with the time-lapse, slow-motion filming).&lt;/p&gt;

&lt;iframe width=&#34;853&#34; height=&#34;480&#34; src=&#34;//www.youtube.com/embed/sA4Za3Hv6ng?rel=0&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Technicaly, the algorithm builds Hyperlapse video in three steps:
 1. &lt;strong&gt;Recover 3D scene&lt;/strong&gt; from the camera motion. This is well-known task called &amp;ldquo;Structure from Motion&amp;rdquo; and one camera is enough to recover 3D environment (Google: Monocular SLAM).
 2. &lt;strong&gt;Optimize route&lt;/strong&gt; (or Path Planning) - on previous step algorithm recover camera route that include shakes, vibration and occasion motions that should be exludede from result Hyperlapse. The goal of this step is to make smooth and stable transition from frame to frame by optimizing route.
 3. ** Render Hyperlapse**. This step doing reverse things - it sample pixel values from all visible frames that were used to reconstruct given pose and pick best ones that produce really nice stiched image. Having 3D environment has a great advantage when algorithm has to &amp;ldquo;inpaint&amp;rdquo; missing reginos - it can sample pixels from the other frames because system reallly knows what is the 3D structure around.&lt;/p&gt;

&lt;p&gt;You can read publication of this approach from the Microsoft Research: &lt;a href=&#34;http://research.microsoft.com/en-us/um/redmond/projects/hyperlapse/paper/hyperlapse.pdf&#34;&gt;First-person Hyper-lapse Videos&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;using-opencv-python-and-template-matching-to-play-where-s-waldo&#34;&gt;Using OpenCV, Python and Template Matching to play “Where’s Waldo?”&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;puzzle_small.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This &lt;a href=&#34;http://machinelearningmastery.com/using-opencv-python-and-template-matching-to-play-wheres-waldo/&#34;&gt;article&lt;/a&gt; is for beginners who start learning computer vision. This tutorial describe very basic, but still powerful technique called template matching for object detection. “Where’s Waldo?” probably the best candidate for template matching demonstration - the task is very clear and this article contain step by step solution on detecting Waldo using computer vision.&lt;/p&gt;

&lt;p&gt;Using Python it&amp;rsquo;s really simple to write your first algorithm:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;puzzle = cv2.imread(args[&amp;quot;puzzle&amp;quot;])
waldo = cv2.imread(args[&amp;quot;waldo&amp;quot;])
result = cv2.matchTemplate(puzzle, waldo, cv2.TM_CCOEFF)
(_, _, minLoc, maxLoc) = cv2.minMaxLoc(result)
# the puzzle image
topLeft = maxLoc
botRight = (topLeft[0] + waldoWidth, topLeft[1] + waldoHeight)
roi = puzzle[topLeft[1]:botRight[1], topLeft[0]:botRight[0]]

# construct a darkened transparent &#39;layer&#39; to darken everything
# in the puzzle except for waldo
mask = np.zeros(puzzle.shape, dtype = &amp;quot;uint8&amp;quot;)
puzzle = cv2.addWeighted(puzzle, 0.25, mask, 0.75, 0)

# put the original waldo back in the image so that he is
# &#39;brighter&#39; than the rest of the image
puzzle[topLeft[1]:botRight[1], topLeft[0]:botRight[0]] = roi

# display the images
cv2.imshow(&amp;quot;Puzzle&amp;quot;, imutils.resize(puzzle, height = 650))
cv2.imshow(&amp;quot;Waldo&amp;quot;, waldo)
cv2.waitKey(0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;puzzle_found_waldo1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Original article can be found here: &lt;a href=&#34;http://machinelearningmastery.com/using-opencv-python-and-template-matching-to-play-wheres-waldo/&#34;&gt;Using OpenCV, Python and Template Matching to play “Where’s Waldo?”&lt;/a&gt;.
&lt;hr /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;opencv-3-0-alpha-is-out&#34;&gt;OpenCV 3.0 alpha is out&lt;/h2&gt;

&lt;p&gt;That&amp;rsquo;s it. OpenCV grows and going to college. 5 years has passed since OpenCV 2.0, which brought us a new C++ API, GPU-accelerated algorithms, iOS and Android platforms support, CUDA and OpenCL, Python and Java bindings.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modular project architecture&lt;/strong&gt;. Since very beginning OpenCV was one solid project, built and shipped as a whole, and that was good strategy for many years. However, with constantly growing functionality, including bleeding-edge algorithms published a few minutes before a pull request has been submitted to our repository, and increasing number of contributors (thank you all very much, guys!) we came to the same conclusion and decision as many other big project – the solid model does not work anymore.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;T-API&lt;/strong&gt;. GPU acceleration made really easy with brand new T-API (“transparent API”) made in cooperation with Intel and AMD. &lt;a href=&#34;https://github.com/Itseez/opencv/tree/master/samples/tapi&#34;&gt;T-API Samples&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;OpenCV now linked with IPP by default&lt;/strong&gt;. Intel corporation gave OpenCV another exciting present. A subset of Intel Integrated Performance Primitives (IPP) is linked by default into OpenCV and is available at &lt;strong&gt;no charge for all our users&lt;/strong&gt;. And that includes the license to redistribute applications that use IPP-accelerated OpenCV. As you may see, for quite a few image processing functions we achieved very noticeable speedup with IPP (where IPP is compared with OpenCV built with all possible optimizations turned on):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;ocv3_ipp_speedup.jpg&#34; alt=&#34;IPP in OpenCV 3.0&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Last but not least, OpenCV 3.0 brings a lot of &lt;strong&gt;new functionality&lt;/strong&gt;, such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Text detection and recognition by Lluis Gomez and Stefano Fabri&lt;/li&gt;
&lt;li&gt;HDR by Fedor Morozov and Alexander Shishkov&lt;/li&gt;
&lt;li&gt;KAZE/A-KAZE by Eugene Khvedchenya, the algorithm author Pablo Alcantarilla and some improvements by F. Morozov.&lt;/li&gt;
&lt;li&gt;Smart segmentation and edge-aware filters by Vitaly Lyudvichenko, Yuri Gitman, Alexander Shishkov and Alexander Mordvintsev&lt;/li&gt;
&lt;li&gt;Car detection using Waldboost, ACF by Vlad Shakhuro and Nikita Manovich&lt;/li&gt;
&lt;li&gt;TLD tracker and several common-use optimization algorithms by Alex Leontiev&lt;/li&gt;
&lt;li&gt;Matlab bindings by Hilton Bristow, with support from Mathworks.&lt;/li&gt;
&lt;li&gt;Greatly extended Python bindings, including Python 3 support, and several OpenCV+Python tutorials by Alexander Mordvintsev, Abid Rahman and others.&lt;/li&gt;
&lt;li&gt;3D Visualization using VTK by Ozan Tonkal and Anatoly Baksheev.&lt;/li&gt;
&lt;li&gt;RGBD module by Vincent Rabaud&lt;/li&gt;
&lt;li&gt;Line Segment Detector by Daniel Angelov&lt;/li&gt;
&lt;li&gt;Many useful Computational Photography algorithms by Siddharth Kherada&lt;/li&gt;
&lt;li&gt;Shape descriptors, matching and morphing shapes (shape module) by Juan Manuel Perez Rua and Ilya Lysenkov&lt;/li&gt;
&lt;li&gt;Long-term tracking + saliency-based improvements (tracking module) by Antonella Cascitelli and Francesco Puja&lt;/li&gt;
&lt;li&gt;Another good pose estimation algorithm and the tutorial on pose estimation by Edgar Riba and Alexander Shishkov&lt;/li&gt;
&lt;li&gt;Line descriptors and matchers by Biagio Montesano and Manuele Tamburanno&lt;/li&gt;
&lt;li&gt;Myriads of improvements in various parts of the library by Steven Puttemans; thank you a lot, Steven!&lt;/li&gt;
&lt;li&gt;Several NEON optimizations by Adrian Stratulat, Cody Rigney, Alexander Petrikov, Yury Gorbachev and others.&lt;/li&gt;
&lt;li&gt;Fast foreach loop over cv::Mat by Kazuki Matsuda&lt;/li&gt;
&lt;li&gt;Image alignment (ECC algorithm) by Georgios Evangelidis&lt;/li&gt;
&lt;li&gt;GDAL image support by Marvin Smith&lt;/li&gt;
&lt;li&gt;RGBD module by Vincent Rabaud&lt;/li&gt;
&lt;li&gt;Fisheye camera model by Ilya Krylov&lt;/li&gt;
&lt;li&gt;OSX framework build script by Eugene Khvedchenya&lt;/li&gt;
&lt;li&gt;Multiple FLANN improvements by Pierre-Emmanuel Viel&lt;/li&gt;
&lt;li&gt;Improved WinRT support by Gregory Morse&lt;/li&gt;
&lt;li&gt;Latent SVM Cascade by Evgeniy Kozhinov and NNSU team (awaiting integration)&lt;/li&gt;
&lt;li&gt;Logistic regression by Rahul Kavi&lt;/li&gt;
&lt;li&gt;Five-point pose estimation algorithm by Bo Li&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The 3.0-alpha package can be downloaded:
 - &lt;a href=&#34;https://github.com/Itseez/opencv/tree/3.0.0-alpha&#34;&gt;Source code as .zip package directly from github&lt;/a&gt;
 - &lt;a href=&#34;https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.0.0-alpha/&#34;&gt;Precompiled, Windows&lt;/a&gt;
 - &lt;a href=&#34;https://sourceforge.net/projects/opencvlibrary/files/opencv-ios/3.0.0-alpha/&#34;&gt;Precompiled, iOS&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mapping data from Eigen to OpenCV and back</title>
      <link>/post/mapping-eigen-to-opencv/</link>
      <pubDate>Sat, 16 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/mapping-eigen-to-opencv/</guid>
      <description>

&lt;div class=&#34;featured-image&#34;&gt;
![Eigen2CV](eigen2cv.png)
&lt;/div&gt;

&lt;p&gt;Eigen is a C++ template library for matrix and vector operations.
It is highly optimized for numeric operations and support vectorization and
use aligned memory allocators.&lt;/p&gt;

&lt;p&gt;When it comes to matrix operations, Eigen is much faster than OpenCV.
However, it can be situations when it is necessary to pass Eigen data
to OpenCV functions.&lt;/p&gt;

&lt;p&gt;In this post I will show how to map Eigen data to OpenCV with easy and efficient
way. No copy, minimal overhead and maximum syntax sugar:
&lt;div class=&#34;clearfix&#34;&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Simple case&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;Eigen::ArrayXXd img(480, 640);
...
cv::imshow(&amp;quot;test&amp;quot;, eigen2cv(img));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Proposed approach does not limited to continuous memory layout - it support expression and blocks
as well. If given expression has to be evaluated - it will be evaluated into temporary dense storage
and then mapped to OpenCV structure:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Expressions&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;// Unsharp mask
Eigen::ArrayXXd img, blur;    
cv::GaussianBlur(eigen2cv(img), eigen2cv(blur));

cv::imshow(&amp;quot;sharpened&amp;quot;, eigen2cv(1.5 * img - 0.5 * blur));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;span class=&#34;more&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;idea&#34;&gt;Idea&lt;/h2&gt;

&lt;p&gt;In fact, Eigen heavily use &lt;a href=&#34;#1&#34;&gt;C++ templates magic&lt;/a&gt; to create expression structures with delayed evaluation and &lt;a href=&#34;#2&#34;&gt;type traits&lt;/a&gt; to detect type of derived objects in compile time.
This approach gives compiler a lot of hints on actual data layout which helps to generate more efficient code.
The drawback of this - if you want to deep dive in Eigen internals be prepared to hardcore.&lt;/p&gt;

&lt;p&gt;I will use templates as well. We will have template class &lt;code&gt;Eigen2CV&lt;/code&gt; and several specializations of this class - for planar types, for blocks, for expression and so on&amp;hellip;
In addition we will specialize this class with mutable specification which will
let us to define &lt;u&gt;at compile time&lt;/u&gt; whether mapped object is allowed for writing or not. Awesome.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;
    typename Derived, 
    typename Base, 
    typename ConstPolicy, 
    typename StorageKind = typename Eigen::internal::traits&amp;lt;Derived&amp;gt;::StorageKind 
    &amp;gt;
class Eigen2CV;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To interact with OpenCV, we can declare implicit conversion operators for &lt;code&gt;cv::Mat&lt;/code&gt;, &lt;code&gt;cv::InputArray&lt;/code&gt; and &lt;code&gt;cv::OutputArray&lt;/code&gt;.
Some of the mapped objects can have read/write access while the rest  - read-only.
Therefore we will introduce the base class &lt;code&gt;Eigen2CVBase&lt;/code&gt; to provide a &amp;ldquo;read-only&amp;rdquo; access for all derived objects.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;class Eigen2CVBase
{
public:
    
    operator cv::Mat() const
    {
        return mBody;
    }
    
    operator cv::_InputArray() const
    {
        return cv::_InputArray(mBody);
    }
    
protected:
    
    template&amp;lt;typename Derived&amp;gt;
    void mapPlaneMemory(const Derived&amp;amp; src)
    {
        const bool isRowMajor = int(Derived::Flags) &amp;amp; Eigen::RowMajorBit;
        const int stride = src.outerStride() * sizeof(typename Derived::Scalar);
        
        if (isRowMajor)
            this-&amp;gt;mapPlaneMemoryRowMajor(src.data(),
                                         src.rows(),
                                         src.cols(),
                                         stride);
        else
            this-&amp;gt;mapPlaneMemoryColMajor(src.data(),
                                         src.rows(),
                                         src.cols(),
                                         stride);
    }

    template &amp;lt;typename Scalar&amp;gt;
    void mapPlaneMemoryRowMajor(const Scalar* planeData, int rows, int cols, int stride)
    {
        this-&amp;gt;mBody = cv::Mat(rows, 
                              cols, 
                              opencv_matrix&amp;lt;Scalar&amp;gt;::type, 
                              const_cast&amp;lt;Scalar*&amp;gt;(planeData), 
                              stride);
    }
    
    template &amp;lt;typename Scalar&amp;gt;
    void mapPlaneMemoryColMajor(const Scalar* planeData, int rows, int cols, int stride)
    {
        this-&amp;gt;mBody = cv::Mat(cols, 
                              rows, 
                              opencv_matrix&amp;lt;Scalar&amp;gt;::type, 
                              const_cast&amp;lt;Scalar*&amp;gt;(planeData), 
                              stride);
    }

    template &amp;lt;typename Derived, typename T&amp;gt;
    void assignMatrix(Eigen::DenseBase&amp;lt;Derived&amp;gt;&amp;amp; dst, const cv::Mat_&amp;lt;T&amp;gt;&amp;amp; src)
    {
        typedef typename Derived::Scalar Scalar;
        typedef Eigen::Matrix&amp;lt;T, 
                              Eigen::Dynamic, 
                              Eigen::Dynamic, 
                              Eigen::RowMajor&amp;gt; PlainMatrixType;
        
        dst = Eigen::Map&amp;lt;PlainMatrixType&amp;gt;((T*)src.data, 
                                          src.rows, 
                                          src.cols).
                                          template cast&amp;lt;Scalar&amp;gt;();
    }
    
    cv::Mat mBody;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For those Eigen types that allows read/write access, we will have additional conversion operator to cv::OutputArray. By default all derived types will have read-only access.&lt;/p&gt;

&lt;h2 id=&#34;mapping-eigen-plain-objects&#34;&gt;Mapping Eigen plain objects&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start from mapping continuous block of memory represented by &lt;code&gt;Eigen::Matrix&lt;/code&gt; of &lt;code&gt;Eigen::Array&lt;/code&gt;.
These two classes derives from &lt;code&gt;Eigen::PlainObjectBase&lt;/code&gt; class which provides methods to access internal storage
buffer.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;typename Derived&amp;gt;
class Eigen2CV&amp;lt;
    Derived, 
    Eigen::PlainObjectBase&amp;lt;Derived&amp;gt;, 
    details::Const&amp;gt; : public Eigen2CVBase
{
public:
    
    typedef typename Derived::Scalar Scalar;
    typedef Eigen2CV&amp;lt;Derived, Eigen::PlainObjectBase&amp;lt;Derived&amp;gt;, details::Mutable&amp;gt; Self;
    
    Eigen2CV(const Eigen::PlainObjectBase&amp;lt;Derived&amp;gt;&amp;amp; src)
    : mMappedView(src)
    {
        this-&amp;gt;mapPlaneMemory(mMappedView);
    }
           
private:
    const Eigen::PlainObjectBase&amp;lt;Derived&amp;gt;&amp;amp; mMappedView;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is &lt;code&gt;Eigen2CV&lt;/code&gt; specialization for constant &lt;code&gt;Eigen::PlainObjectBase&lt;/code&gt; object. This specialization of &lt;code&gt;Eigen2CV&lt;/code&gt; can return constant reference to &lt;code&gt;cv::Mat&lt;/code&gt; and &lt;code&gt;cv::InputArray&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now we can write two overloads of &lt;code&gt;eigen2cv&lt;/code&gt; function for &lt;code&gt;Eigen::Matrix&lt;/code&gt; and &lt;code&gt;Eigen::Array&lt;/code&gt;. The goal of &lt;code&gt;eigen2cv&lt;/code&gt; is simple - take an argument and create &amp;lsquo;right&amp;rsquo; Eigen2CV&amp;lt;&amp;hellip;&amp;gt; mapper.
Here is how it looks like for planar data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;typename E&amp;gt;
Eigen2CV&amp;lt;E, Eigen::PlainObjectBase&amp;lt;E&amp;gt;, details::Mutable&amp;gt; 
eigen2cv(Eigen::PlainObjectBase&amp;lt;E&amp;gt;&amp;amp; src) 
{
    return Eigen2CV&amp;lt;E, 
                    Eigen::PlainObjectBase&amp;lt;E&amp;gt;, 
                    details::Mutable
                    &amp;gt;(src));
}

template&amp;lt;typename E&amp;gt;
Eigen2CV&amp;lt;E, Eigen::PlainObjectBase&amp;lt;E&amp;gt;, details::Const&amp;gt;
eigen2cv(const Eigen::PlainObjectBase&amp;lt;E&amp;gt;&amp;amp; src) 
{
    return Eigen2CV&amp;lt;E, 
                    Eigen::PlainObjectBase&amp;lt;E&amp;gt;, 
                    details::Const
                    &amp;gt;(src);
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I want to draw your attention to how elegant C++ allows us to distinct mutable and constant objects.
Compiler will choose right function depending on the context of &lt;code&gt;src&lt;/code&gt;.
In case of access right conflicts you will get compile-time error.&lt;/p&gt;

&lt;h2 id=&#34;assigning-opencv-matrix-to-eigen-object&#34;&gt;Assigning OpenCV matrix to Eigen object&lt;/h2&gt;

&lt;p&gt;What if someone write:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;eigen2cv(a) = cv::imread(&amp;quot;lena.jpg&amp;quot;, cv::IMREAD_GRAYSCALE);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Well, it&amp;rsquo;s legal and I see no problems with this code while we follow few restrictions:
1. &lt;code&gt;data&lt;/code&gt; has dynamic size or fixed one which match cv::Mat size.
2. Image is single channel - there is no way to map multi-channel images to Eigen now.&lt;/p&gt;

&lt;p&gt;Assignment operator is also quite simple:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template &amp;lt;typename T&amp;gt;
Self&amp;amp; operator=(const cv::Mat_&amp;lt;T&amp;gt;&amp;amp; src)
{
    assignMatrix&amp;lt;Derived, T&amp;gt;(mMappedView, src);
    return *this;
}

/**
 * @brief Assignment operator to copy OpenCV Mat data to mapped Eigen object.
 */
Self&amp;amp; operator= (const cv::Mat&amp;amp; m)
{
    switch (m.type())
    {
        case CV_8U:  return *this = (cv::Mat_&amp;lt;uint8_t&amp;gt;)m;
        case CV_16U: return *this = (cv::Mat_&amp;lt;uint16_t&amp;gt;)m;
        case CV_16S: return *this = (cv::Mat_&amp;lt;int16_t&amp;gt;)m;
        case CV_32S: return *this = (cv::Mat_&amp;lt;int32_t&amp;gt;)m;
        case CV_32F: return *this = (cv::Mat_&amp;lt;float&amp;gt;)m;
        case CV_64F: return *this = (cv::Mat_&amp;lt;double&amp;gt;)m;
        default:
            throw std::runtime_error(&amp;quot;Unsupported OpenCV matrix type&amp;quot;);
    };
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mapping-eigen-expressions&#34;&gt;Mapping Eigen expressions&lt;/h2&gt;

&lt;p&gt;Dealing with expressions is not much harder.
Depending on the expression type, we must either evaluate it into dense storage (when it&amp;rsquo;s real expression like &lt;code&gt;AX + B&lt;/code&gt;, or &lt;code&gt;cast&amp;lt;float&amp;gt;()&lt;/code&gt;)
or use underlying storage with regards to expression operator (&lt;code&gt;block()&lt;/code&gt;, &lt;code&gt;transpose()&lt;/code&gt;, &lt;code&gt;array()&lt;/code&gt;, &lt;code&gt;matrix()&lt;/code&gt;).
We will get to mapping blocks in a next section.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s map expression that require evaluation first. For the sake of simplicity,
I will not implement write expressions, e.g expressions that require eval/update/write-back.
Eigen2CV will be able map Eigen expressions in read-only mode.
And here&amp;rsquo;s how:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;typename Derived&amp;gt;
class Eigen2CV&amp;lt;Derived, Eigen::EigenBase&amp;lt;Derived&amp;gt;, details::Const&amp;gt; : public Eigen2CVBase
{
public:
    typedef typename Derived::Scalar Scalar;
    typedef typename Eigen::internal::plain_matrix_type&amp;lt;Derived&amp;gt;::type StorageType;

    Eigen2CV(const Eigen::EigenBase&amp;lt;Derived&amp;gt;&amp;amp; src)
    {
        mStorage = src; // All magic happens here
        this-&amp;gt;mapPlaneMemory(mStorage);
    }

protected:

    void mapPlaneMemory(StorageType&amp;amp; src)
    {
        if ( ( StorageType::Options &amp;amp; Eigen::RowMajor) == Eigen::RowMajor)
            this-&amp;gt;mapPlaneMemoryRowMajor(src.data(), 
                                         src.rows(), 
                                         src.cols(), 
                                         src.outerStride() * sizeof(Scalar));
        else
            this-&amp;gt;mapPlaneMemoryColMajor(src.data(), 
                                         src.rows(), 
                                         src.cols(), 
                                         src.outerStride() * sizeof(Scalar));
    }

private:
    StorageType mStorage;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the first look, it is almost the same as specialization for planar data types. A few differences make this specialization very other one.
First, &lt;code&gt;Eigen::internal::plain_matrix_type&amp;lt;Derived&amp;gt;::type&lt;/code&gt; type trait helps us to infer type of dense storage for given expression.
Second, line &lt;code&gt;mStorage = src&lt;/code&gt; looks really simple right? But hold on, &lt;code&gt;src&lt;/code&gt; is an expression, and &lt;code&gt;mStorage&lt;/code&gt; is dense matrix.
Assignment operator makes our like much easier by performing evaluation step inside this assignment.&lt;/p&gt;

&lt;p&gt;And here is &lt;code&gt;eigen2cv&lt;/code&gt; overload for Eigen expressions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;typename E&amp;gt;
Eigen2CV&amp;lt;E, Eigen::EigenBase&amp;lt;E&amp;gt;, details::Const&amp;gt;
eigen2cv(const Eigen::EigenBase&amp;lt;E&amp;gt;&amp;amp; src) 
{
    return Eigen2CV&amp;lt;E, 
                    Eigen::EigenBase&amp;lt;E&amp;gt;, 
                    details::Const
                    &amp;gt;(src);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mapping-eigen-views&#34;&gt;Mapping Eigen views&lt;/h2&gt;

&lt;p&gt;User can create sub-view for the Eigen storage using &lt;code&gt;block()&lt;/code&gt;.
Eigen block create view that points to the same memory region, but has different size and stride.
Blocks can be read and written.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;typename Derived&amp;gt;
class Eigen2CV&amp;lt;Derived, 
               Eigen::Block&amp;lt;Derived&amp;gt;, 
               details::Mutable, 
               Eigen::Dense&amp;gt; : public Eigen2CVBase
{
public:
    typename Derived::Scalar Scalar;
    typedef Eigen2CV&amp;lt;Derived, Eigen::Block&amp;lt;Derived&amp;gt;, details::Mutable&amp;gt; Self;

    Eigen2CV(const Eigen::Block&amp;lt;Derived&amp;gt;&amp;amp; src)
        : mMappedView(src)
    {
        this-&amp;gt;mapPlaneMemory(mMappedView);
    }
    
    operator cv::_OutputArray()
    {
        return cv::_OutputArray(this-&amp;gt;mBody);
    }
    
    template &amp;lt;typename T&amp;gt;
    Self&amp;amp; operator=(const cv::Mat_&amp;lt;T&amp;gt;&amp;amp; src)
    {
        MatrixAssign&amp;lt;Derived, T&amp;gt;(mMappedView, src);
        return *this;
    }
    
    /**
     * @brief Assignment operator to copy OpenCV Mat data to mapped Eigen object.
     */
    Self&amp;amp; operator= (const cv::Mat&amp;amp; m) throw ()
    {
        switch (m.type())
        {
            case CV_8U:  return *this = (cv::Mat_&amp;lt;uint8_t&amp;gt;)m;
            case CV_16U: return *this = (cv::Mat_&amp;lt;uint16_t&amp;gt;)m;
            case CV_16S: return *this = (cv::Mat_&amp;lt;int16_t&amp;gt;)m;
            case CV_32S: return *this = (cv::Mat_&amp;lt;int32_t&amp;gt;)m;
            case CV_32F: return *this = (cv::Mat_&amp;lt;float&amp;gt;)m;
            case CV_64F: return *this = (cv::Mat_&amp;lt;double&amp;gt;)m;
            default:
                throw std::runtime_error(&amp;quot;Unsupported OpenCV matrix type&amp;quot;);
        };
    }
    
private:
    const Eigen::Block&amp;lt;Derived&amp;gt;&amp;amp; mMappedView;
};

template&amp;lt;typename E&amp;gt;
Eigen2CV&amp;lt;E, Eigen::Block&amp;lt;E&amp;gt;, details::Mutable&amp;gt;
eigen2cv(const Eigen::Block&amp;lt;E&amp;gt;&amp;amp; src)
{
    return Eigen2CV&amp;lt;E, 
                    Eigen::Block&amp;lt;E&amp;gt;, 
                    details::Mutable
                    &amp;gt;(src);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;eigen-memory-organization&#34;&gt;Eigen memory organization&lt;/h2&gt;

&lt;p&gt;Eigen can use column-major or row-major ordering of internal data storage.
By default it&amp;rsquo;s column-major, but OpenCV use row-major ordering.&lt;/p&gt;

&lt;p class=&#34;info info-warning&#34;&gt;
&lt;span class=&#34;label label-info&#34;&gt;Notice&lt;/span&gt;
This mapping implementation will NOT convert underlying Eigen memory to meet OpenCV convention. 
For column-major order of Eigen data type this will lead to transposed matrices in OpenCV. 
&lt;/p&gt;

&lt;h2 id=&#34;demonstration&#34;&gt;Demonstration&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;Image8u_t a(512, 512); // Eigen::Matrix&amp;lt;uint8_t, Dynamic, Dynamic&amp;gt;

for (size_t i = 0; i &amp;lt; 512; i++)
{
    for (size_t j = 0; j &amp;lt; 512; j++)
    {
        a(i,j) = 255.0f * (sin(0.04f * i) * sin(0.04f * i) + 
                           cos(0.04f * j) * cos(0.04f * j));
    }
}

cv::GaussianBlur(eigen2cv(a.block(128, 128, 256, 256)),
                 eigen2cv(a.block(128, 128, 256, 256)), cv::Size(25,25), 0);
cv::imshow(&amp;quot;Blur image region&amp;quot;, eigen2cv(a));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;blur_roi.png&#34; alt=&#34;Blur image region&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;source-code&#34;&gt;Source code&lt;/h2&gt;

&lt;p&gt;Source code for this post can be found on GitHub: &lt;a href=&#34;https://gist.github.com/BloodAxe/c94d65d5977fb1d3e53f&#34;&gt;Eigen2CV.h&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;License: &lt;a href=&#34;https://tldrlegal.com/license/bsd-3-clause-license-(revised)#summary&#34;&gt;BSD-3&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a name=&#34;#1&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.amazon.com/gp/product/0201704315/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0201704315&amp;linkCode=as2&amp;tag=compvisitalk-20&amp;linkId=2ZA2JDQNEDOQJZFL&#34;&gt;Modern C++ Design: Generic Programming and Design Patterns Applied&lt;/a&gt;&lt;img src=&#34;http://ir-na.amazon-adsystem.com/e/ir?t=compvisitalk-20&amp;l=as2&amp;o=1&amp;a=0201704315&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a name=&#34;#2&#34; /&gt;
2. &lt;a href=&#34;http://www.drdobbs.com/cpp/c-type-traits/184404270&#34;&gt;http://www.drdobbs.com/cpp/c-type-traits/184404270&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A good resource on image processing using Python</title>
      <link>/post/pyimagesearch.com/</link>
      <pubDate>Thu, 07 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/pyimagesearch.com/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.pyimagesearch.com/&#34;&gt;&lt;img src=&#34;logo.png&#34; alt=&#34;www.pyimagesearch.com&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let me introduce you &lt;a href=&#34;http://www.pyimagesearch.com/about/&#34;&gt;Adrian Rosebrock&lt;/a&gt; and his &lt;a href=&#34;http://www.pyimagesearch.com/&#34;&gt;http://www.pyimagesearch.com/&lt;/a&gt; website.
It&amp;rsquo;s about computer vision and image processing using Python and OpenCV.
Looks like there are more than one person that like to share programming experience via blogging :)&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s how &lt;a href=&#34;http://www.pyimagesearch.com/&#34;&gt;Adrian&lt;/a&gt; position himself:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This blog is dedicated to helping other programmers understand how image search engines work.
While a lot of computer vision concepts are theoretical in nature,
I’m a big fan of “learning by example”. My goal is to distill my life experiences in building image search engines into concise, easy to understand examples.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I hope you will enjoy reading Adrian&amp;rsquo;s posts on &lt;a href=&#34;http://www.pyimagesearch.com/2014/07/28/a-slic-superpixel-tutorial-using-python/&#34;&gt;superpixels&lt;/a&gt;, &lt;a href=&#34;http://www.pyimagesearch.com/2014/07/14/3-ways-compare-histograms-using-opencv-python/&#34;&gt;histogram matching&lt;/a&gt; and &lt;a href=&#34;http://www.pyimagesearch.com/2014/05/26/opencv-python-k-means-color-clustering/&#34;&gt;color clustering&lt;/a&gt;.
In addition, he wrote a book on using OpenCV in Python.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.pyimagesearch.com/practical-python-opencv/&#34;&gt;&lt;img src=&#34;practical_python_and_opencv_cover_green.png&#34; alt=&#34;Practical Python and OpenCV eBook&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Happy reading!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to detect circles in noisy images</title>
      <link>/post/how-to-detect-circles-in-noisy-image/</link>
      <pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/how-to-detect-circles-in-noisy-image/</guid>
      <description>&lt;p&gt;p
    | This was a request from
    a(href=&amp;ldquo;&lt;a href=&#34;http://www.reddit.com/r/computervision/comments/2a1lvi/help_how_to_process_this_image_to_find_the_circles/&amp;quot;&#34;&gt;http://www.reddit.com/r/computervision/comments/2a1lvi/help_how_to_process_this_image_to_find_the_circles/&amp;quot;&lt;/a&gt;) /r/computervision.
    | A reddit member was asking on how to count number of eggs on quite
    | noisy image like you may see below.
    | I&amp;rsquo;ve decided to write a simple algorithm that does the job and explain how it works.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;div.beforeafter
    img(src=&amp;quot;source.jpg&amp;quot;,alt=&amp;quot;before&amp;quot;)
    img(src=&amp;quot;display.jpg&amp;quot;,alt=&amp;quot;after&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;span.more&lt;/p&gt;

&lt;p&gt;h2 Step 1 - Filter image&lt;/p&gt;

&lt;p&gt;p
    img(src=&amp;ldquo;source.jpg&amp;rdquo;,alt=&amp;ldquo;Source image&amp;rdquo;)
    | The original image has noticeable color noise and therefore it must be filtered before we pass it to further stages.
    | Ideally you should choose filter algorithm based on your task and noise model. For the sake of simplicity,
    | I will not use Weiner filter or deconvolution to deal with blur and artifacts that are present on source image.&lt;/p&gt;

&lt;p&gt;p
    | Instead, I will use pyramidal mean-shift filter.
    | This algorithm works quite well on this problem and helps to get rid of artifacts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pre.
    cv::Mat inputRgbImage = cv::imread(&amp;quot;input.jpg&amp;quot;);
    cv::Mat filtered;
    cv::pyrMeanShiftFiltering(inputRgbImage, 
                              filtered, 
                              spatialWindowRadius, 
                              colorWindowRadius, 
                              2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;strong Result of filtering
div.beforeafter.twentytwenty-container
    img(src=&amp;ldquo;source.jpg&amp;rdquo;,alt=&amp;ldquo;before&amp;rdquo;)
    img(src=&amp;ldquo;filtered.jpg&amp;rdquo;,alt=&amp;ldquo;after&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;h2 Step 2 - Increase sharpness&lt;/p&gt;

&lt;p&gt;p
    | As you may see - eggs edges are not too sharp. For circle detection we will use Hough transform. OpenCV&amp;rsquo; Hough algorithm implementation use Canny edge detector to
    | detect edges. Unfortunately it&amp;rsquo;s not possible to pass manually computed binary image.
    | Instead we have to pass 8-bit grayscale image for circle detection.
    | Therefore we may want to increase their sharpness to make the image more friendly for Canny detector.&lt;/p&gt;

&lt;p&gt;p
    | Sharpening can be easily done via unsharp mask.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pre.
    // Perform in-place unsharp masking operation
    // http://opencv-code.com/quick-tips/sharpen-image-with-unsharp-mask/
    void unsharpMask(cv::Mat&amp;amp; im) 
    {
        cv::Mat tmp;
        cv::GaussianBlur(im, tmp, cv::Size(5,5), 5);
        cv::addWeighted(im, 1.5, tmp, -0.5, 0, im);
    }

| However, unsharp mask can cause artifacts on edge borders which may lead to double edge.
| I&#39;m dealing with it by adding laplaccian component to final result:

pre.
    cv::cvtColor(filtered, grayImg, cv::COLOR_BGR2GRAY);
    grayImg.convertTo(grayscale, CV_32F);

    cv::GaussianBlur(grayscale, blurred, cv::Size(5,5), 0);

    cv::Laplacian(blurred, laplaccian, CV_32F);

    cv::Mat sharpened = 1.5f * grayscale
                      - 0.5f * blurred
                      - weight * grayscale.mul(scale * laplaccian);

strong Result of sharpening
div.beforeafter.twentytwenty-container
    img(src=&amp;quot;filtered.jpg&amp;quot;,alt=&amp;quot;before&amp;quot;)
    img(src=&amp;quot;filteredGray.jpg&amp;quot;,alt=&amp;quot;after&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;h3 Step 3 - Detect circles&lt;/p&gt;

&lt;p&gt;p
    | After we got nicely filtered and enchanced image we can pass it to cv::HoughCircles to detect circles on the image.
    | According to problem task, we can limit the maximum radius of circles that we are interested in (we need only small circles).
    | In addition eggs can be close to each other, so it make sense to set minimal distance between two circles to minimal
    | allowed diameter of the circle.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pre.
   cv::HoughCircles(filteredGray, 
                    circles, 
                    cv::HOUGH_GRADIENT, 
                    2,   // Accumulator resolution
                    12,  // Minimum distance between the centers of the detected circles.
                    cannyThreshold, 
                    accumulatorThreshold, 
                    5,   // Minimum circle radius
                    20); // Maximum circle radius
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;h4 Step 4 - Validate eggs&lt;/p&gt;

&lt;p&gt;p
    | I&amp;rsquo;ve skipped validation step since I don&amp;rsquo;t know what are the requirements to detected eggs.
    | But I suppose that after detection of possible candidates using Hough the algorithm should validate each contour
    | to verify it is exactly what we were looking for. Here are some hints what we can check:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ul
    li Contour defects - how egg shape is close to ideal circle
    li Contour breaks - is there any breaks in egg boundary or not
    li Egg color - perhaps we need to count objects only of particular color
    li Neighbours - maybe (or maybe not) we need only isolated objects.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;h4 Step 5 - GPU Speed-up&lt;/p&gt;

&lt;p&gt;p
    | The given implementation is very slow since it use CPU and doing a lot of stuff that can be efficiently computed on GPU.
    | Fortunately, OpenCV has GPU implementations for mean-shift segmentation and hough transform and image blending.
    | You can easily speed-up this algorithm by using cv::gpu types and functions instead.&lt;/p&gt;

&lt;p&gt;h4 Bonus content&lt;/p&gt;

&lt;p&gt;p
    | Readed to the end of article? I&amp;rsquo;m impressed :) Here you go, the full source code of the algorithm and small playground to
    | tweak setting in runtime:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;script(src=&amp;quot;https://gist.github.com/BloodAxe/943fb14220021113d405.js&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Computer vision Digest - June 2014</title>
      <link>/post/2014-06-computer-vision-digest/</link>
      <pubDate>Sat, 05 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014-06-computer-vision-digest/</guid>
      <description>

&lt;p&gt;This is a second issue of monthly computer vision digest - a list things that
you don&amp;rsquo;t wanna miss, a list of what happened in computer vision in June 2014.&lt;/p&gt;

&lt;p&gt;Previous issues:
 - &lt;a href=&#34;/articles/2014-05-computer-vision-digest/&#34;&gt;Computer Vision Digest (May 2014)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this issue:
 - &lt;a href=&#34;#1&#34;&gt;Signed Distance Field - converting raster masks to vector form&lt;/a&gt;
 - &lt;a href=&#34;#2&#34;&gt;QVision: Computer Vision Library for Qt&lt;/a&gt;
 - &lt;a href=&#34;#3&#34;&gt;Closer look on licence plate recognition&lt;/a&gt;
 - &lt;a href=&#34;#4&#34;&gt;OpenCV 3.0&lt;/a&gt;&lt;/p&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
Feel free to leave your suggestions on interesting materials in post comments 
or via Twitter by mentioning [@cvtalks](https://twitter.com/cvtalks). 
Best links will be included into next digest!
&lt;/div&gt;

&lt;p&gt;&lt;span class=&#34;more clearfix&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;1&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;signed-distance-field-converting-raster-masks-to-vector-form&#34;&gt;Signed Distance Field - converting raster masks to vector form&lt;/h1&gt;

&lt;p&gt;The original paper written in Russian, but the topic is rather interesting.
It describe how to render high-resolution &amp;ldquo;vector&amp;rdquo; graphics from small raster images.
That&amp;rsquo;s why I decided to include this into digest.&lt;/p&gt;

&lt;p&gt;The key algorithm that allows to convert raster mask to vector repesentation
form is &lt;a href=&#34;http://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html#distancetransform&#34;&gt;distance transform&lt;/a&gt; - algorithm, which calculates
distance from every binary image pixel to the nearest zero pixel.&lt;/p&gt;

&lt;p&gt;Consider following example:
&lt;img src=&#34;41bad64c88d9a859d2ba0eb3b7b437bf.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The SDF image we compute from original image can be significatly scaled down to, but it still can be used to render
image with large zoom without aliasing artifacts that typical for raster rendering.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;56cc184627964797b10b34687180a24b.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Paper in russian: &lt;a href=&#34;http://habrahabr.ru/post/215905/&#34;&gt;Signed Distance Field или как сделать из растра вектор&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This algorithm was developed by Valve and presented at SIGGRAPH 2007. You can read original paper:
&lt;a href=&#34;http://www.valvesoftware.com/publications/2007/SIGGRAPH2007_AlphaTestedMagnification.pdf&#34;&gt;Improved Alpha-Tested Magniﬁcation for Vector Textures and Special Effects&lt;/a&gt;. Special thanks to @jin for the link.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;2&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;qvision-computer-vision-library-for-qt&#34;&gt;QVision: Computer Vision Library for Qt&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;qvisionpenguin.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;http://qvision.sourceforge.net/index.html&#34;&gt;QVision&lt;/a&gt; is a free and open source library oriented to the development of computer vision, image/video processing, and scientific computing applications. It is based on the Qt application framework, so it is an object-oriented and cross-platform library for C++.&lt;/p&gt;

&lt;p&gt;The library is mainly intended for educational and research purposes, usability and performance. It has a clean and well documented, Qt-style, object oriented API, which provides functionality for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Video and image input/output.&lt;/li&gt;
&lt;li&gt;Image processing.&lt;/li&gt;
&lt;li&gt;Graphical interface programming.&lt;/li&gt;
&lt;li&gt;Augmented reality visualization.&lt;/li&gt;
&lt;li&gt;Performance evaluation.&lt;/li&gt;
&lt;li&gt;Scientific computing (matrix, vector, quaternions, function optimization, etc..).&lt;/li&gt;
&lt;li&gt;Visual data-path editor tool for rapid application development (RAD).&lt;/li&gt;
&lt;li&gt;&amp;hellip; and so on.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I liked this library because it allows to desing algorithm using graph concept:
&lt;img src=&#34;cannyBlockExample.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Visual designer allows you to connect data sources with image filters that transform one source
to another and connect filters in a chain to build a computer vision pipeline without wiring
any single line of code.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;qvdesignergui.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I beleive this kind of playground can be very useful for fast prototyping and learning basics
of computer vision. The visual designer does not require any knowledge of any computer vision
library (like OpenCV or Halcon).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;hartley-combined-edge-movement-detector.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Project homepage: &lt;a href=&#34;http://qvision.sourceforge.net/index.html&#34;&gt;QVision&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;3&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;closer-look-on-licence-plate-recognition&#34;&gt;Closer look on licence plate recognition&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;d29e20441d4ab164a5fe13f881b684ce.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Again, another publication in Russian, but hey, it&amp;rsquo;s still worth reading it, even via Google Translate.
License plate recognition is very demanded topic and there are many systems for that. But what about
knowledge sharing? Guys from Recognitor share their experience with recognizing plate numbers in very,
very unfriendly conditions - dirty numbers, dark and blurred.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;0d94a205fe806c8d57660ba35188df27.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here are key features of their implementation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Small rotation invariance to plate rotation (± 10 degree)&lt;/li&gt;
&lt;li&gt;Perspective scale invariance (20%)&lt;/li&gt;
&lt;li&gt;Robustness to partial occlusion of the license plate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Usually, the first step in system like that is image binarisation. This works fine when we have a clean number and
friendly lighting conditions. In other cases this method does not help at all. A better approach is to find top and
bottom lines of the plate using brightness histogram.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Detect bottom border&lt;/li&gt;
&lt;li&gt;Detect top border&lt;/li&gt;
&lt;li&gt;Detect left and right borders&lt;/li&gt;
&lt;li&gt;Increase contrast in ROI&lt;/li&gt;
&lt;li&gt;Split symbols&lt;/li&gt;
&lt;li&gt;Symbol matching&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To attract your attention - here is an example of what their algorihtm is capable to recognize:
&lt;img src=&#34;74314a4e67ab06faac8f1f5706433e33.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Impressed? So am I was. Original post: &lt;a href=&#34;http://habrahabr.ru/company/recognitor/blog/225913/&#34;&gt;Распознавание автомобильных номеров в деталях&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;opencv-3-0&#34;&gt;OpenCV 3.0&lt;/h1&gt;

&lt;p&gt;No, it has not yet released. But if you build latest revision of master branch, CMake will happily report:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;OpenCV ARCH: x86
OpenCV RUNTIME: vc12
OpenCV STATIC: ON
Found OpenCV 3.0.0 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you may read in my blog I &lt;a href=&#34;/articles/kaze-1.6-in-opencv/&#34;&gt;ported&lt;/a&gt; of KAZE features to OpenCV. And I proud that
my contribution will be a part of next OpenCV release.&lt;/p&gt;

&lt;p&gt;OpenCV team has not made any announcement about 3.0 release date. Personally I&amp;rsquo;d expect it
to happed at the end of GSoC. So let&amp;rsquo;s keep fingers crossed.&lt;/p&gt;

&lt;p&gt;Meanwhile, here is a presentation that can reveal some details of what you can expect from OpenCV 3.0:&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/36806594&#34; width=&#34;800&#34; height=&#34;600&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Computer vision Digest - May 2014</title>
      <link>/post/2014-05-computer-vision-digest/</link>
      <pubDate>Fri, 30 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014-05-computer-vision-digest/</guid>
      <description>

&lt;p&gt;This is a first issue of monthly computer vision digest - a list things that you don&amp;rsquo;t wanna miss, a list of what happened
in computer vision in May 2014.&lt;/p&gt;

&lt;p&gt;In this issue:
 - &lt;a href=&#34;#1&#34;&gt;Browser image processing - how fast is it?&lt;/a&gt;
 - &lt;a href=&#34;#2&#34;&gt;Object recognition using neural networks via JavaScript&lt;/a&gt;
 - &lt;a href=&#34;#3&#34;&gt;NASA shares it&amp;rsquo;s own computer vision library&lt;/a&gt;
 - &lt;a href=&#34;#4&#34;&gt;Easy optimization of image processing pipelines using decoupling algorithms&lt;/a&gt;
 - &lt;a href=&#34;#5&#34;&gt;OpenCV Apparel Store&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more clearfix&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;1&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;browser-image-processing-how-fast-is-it&#34;&gt;Browser image processing - how fast is it?&lt;/h2&gt;

&lt;p&gt;Real-time image processing gets more and more demanded and popular since computer power grows and now it&amp;rsquo;s possible to decode video, apply additional filters in real-time to play HD video smoothly. On desktop platforms this is not a &amp;ldquo;wow&amp;rdquo; anymore. In contrast, in web we cannot brag such results. So, Russian-speaking readers can stop reading here and visit original article: &lt;a href=&#34;http://habrahabr.ru/post/221619/&#34;&gt;Оценка возможности постобработки видео в браузере&lt;/a&gt;. For others I wrote a translation of this post in a free form with my 5 cents.&lt;/p&gt;

&lt;p&gt;Strictly speaking there are two options at the moment: either we use JavaScripts (pure JS, Asm.js or SIMD.js) or apply WebGL to utilize GPU.&lt;/p&gt;

&lt;h3 id=&#34;javascript-way&#34;&gt;JavaScript way&lt;/h3&gt;

&lt;p&gt;The image processing in JavaScript using Canvas.getImageData is slow like hell. On 1080p frames, getting frame-buffer from canvas can take up to 30ms on the desktop.
Regardless of the JavaScript runtime performance, the limiting factor for large images is Canvas - at the moment it is not optimized for frequent read/write access.&lt;/p&gt;

&lt;p&gt;This is not all bad news - JavaScript is slow regardless of Canvas. Simple 3x3 box blur needs approximately 400ms to process single image frame:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function blur(source, width, height) {
    function blur_core(ptr, offset, stride) {
        return (ptr[offset - stride - 4] +
                ptr[offset - stride] +
                ptr[offset - stride + 4] +
                ptr[offset - 4] +
                ptr[offset] +
                ptr[offset + 4] +
                ptr[offset + stride - 4] +
                ptr[offset + stride] +
                ptr[offset + stride + 4]
                ) / 9;
    }

    var stride = width * 4;
    for (var y = 1; y &amp;lt; (height - 1); ++y) {
        var offset = y * stride;
        for (var x = 1; x &amp;lt; stride - 4; x += 4) {
            source[offset] = blur_core(source, offset, stride);
            source[offset + 1] = blur_core(source, offset + 1, stride);
            source[offset + 2] = blur_core(source, offset + 2, stride);
            offset += 4;
        }
    }
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;asm-js&#34;&gt;asm.js&lt;/h3&gt;

&lt;p&gt;This JavaScript subset from Mozilla pretends to be extraordinarily optimizable code. This sub-language effectively describes a safe virtual machine for memory-unsafe languages like C or C++. A combination of static and dynamic validation allows JavaScript engines to employ an ahead-of-time (AOT) optimizing compilation strategy for valid asm.js code.&lt;/p&gt;

&lt;p&gt;Well, it&amp;rsquo;s hard to say writing asm.js code is fun. Be prepared to write code like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (i = 1; (i | 0) &amp;lt; (ntp | 0); i = (i | 0) + 1 | 0) {
    // tp[i] = 2 * tp[i - 1]
    tp[(i &amp;lt;&amp;lt; 3) &amp;gt;&amp;gt; 3] = +(+2 * tp[((i - 1) &amp;lt;&amp;lt; 3) &amp;gt;&amp;gt; 3]);
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function f(x, y) {
    // SECTION A: parameter type declarations
    x = x|0;      // int parameter
    y = +y;       // double parameter

    // SECTION B: function body
    log(x|0);     // call into FFI -- must force the sign
    log(y);       // call into FFI -- already know it&#39;s a double
    x = (x+3)|0;  // signed addition

    // SECTION C: unconditional return
    return ((((x+1)|0)&amp;gt;&amp;gt;&amp;gt;0)/(x&amp;gt;&amp;gt;&amp;gt;0))|0; // compound expression
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And all you get for this hard to read code is 2x speed-up. Not impressed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;macro4b.png&#34; alt=&#34;Asm.js performance&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;simd-js&#34;&gt;SIMD.js&lt;/h3&gt;

&lt;p&gt;This one works only in Firefox Nightly builds and Chrome, and utilize parallelism to deliver high performance within a constrained power budget.  Through Single Instruction, Multiple Data (SIMD) instructions, processors exploit the fine-grained parallelism in applications by simultaneously processing the same operation on multiple data items, delivering major performance improvements at high power efficiency. SIMD is particularly applicable to common computations in image/audio/video processing including computer vision and perceptual computing.&lt;/p&gt;

&lt;p&gt;This library promised up for 400% speedups using floating-point computing:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;simd_in_firefox-623x261.jpg&#34; alt=&#34;SIMD.js in Firefox&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The API looks much better:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image003_0.png&#34; alt=&#34;API&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Did I mention that SIMD.js was made by &lt;a href=&#34;https://01.org/node/1495&#34;&gt;Intel&lt;/a&gt;?&lt;/p&gt;

&lt;h3 id=&#34;webgl&#34;&gt;WebGL&lt;/h3&gt;

&lt;p&gt;This is orthogonal to what we considered before. WebGL is an API to GPU, and native OpenGL driver. Initially it was intented to be used in 3G graphics and gaming, but no one can prevent you to utilize General Purpose GPU (GPGPU) for image processing. Unfortunately, you will have to write shaders in very limited GLSL shading language which lacks of many cool features that are present in CUDA or OpenCL. But still, it is much faster than CPU way.&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;At this moment, the only reasonable technology you may want to consider for real-time image processing is WebGL. You can think about CPU image processing only if you need to process small images or there is no neeed to fit in real-time.
&lt;hr /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;2&#34; &gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;object-recognition-using-neural-networks-via-javascript&#34;&gt;Object recognition using neural networks via JavaScript&lt;/h2&gt;

&lt;p&gt;It can take years to master neural networks. Researchers spend enormous amount of time and efforts to study them and train networks to make them remember, predict and learn.
Neural networks are good for object recognition purpose when you have a fixed set of objects you want to recognize. In this sense they are like Haar Cascades. In contrast, NN can distinguis between 1000 object categories, while Haar Cascade classifier used to detect a single kind of object that can vary (The most popular use of cascades - face detection).&lt;/p&gt;

&lt;p&gt;To demonstrate you the potential of NN, here is one example from &lt;a href=&#34;http://www.image-net.org/challenges/LSVRC/2012/results.html&#34;&gt;Large Scale Visual Recognition Challenge 2012&lt;/a&gt;:
The SuperVision team won the &lt;em&gt;first place&lt;/em&gt; using deep convolutional neural network trained on raw RGB  pixel values. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three globally-connected layers with a final 1000-way softmax.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;It was trained on two NVIDIA GPUs for about a week.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is the price NN users pay for quality. But it worth of it.&lt;/p&gt;

&lt;p&gt;Recenty I came across the interesting project that offers you a read-to-use implementation of similar implementation of Krizhevsky architecture in form of SDK for iOS, Android and even JavaScript!&lt;/p&gt;

&lt;p&gt;&lt;iframe src=&#34;//player.vimeo.com/video/91460768&#34; width=&#34;500&#34; height=&#34;281&#34; frameborder=&#34;0&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt; &lt;p&gt;&lt;a href=&#34;http://vimeo.com/91460768&#34;&gt;Deep Belief SDK Demo&lt;/a&gt; from &lt;a href=&#34;http://vimeo.com/petewarden&#34;&gt;Pete Warden&lt;/a&gt; on &lt;a href=&#34;https://vimeo.com&#34;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;So, please welcome &lt;a href=&#34;https://www.jetpac.com/developer&#34;&gt;Deep Belief SDK&lt;/a&gt;.
&lt;hr /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;3&#34; &gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;nasa-shares-it-s-own-computer-vision-library&#34;&gt;NASA shares it&amp;rsquo;s own computer vision library&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;Cev_launch.jpg&#34; alt=&#34;NASA ASR&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The NASA Vision Workbench (VW) is a general purpose image processing and computer vision library developed by the Autonomous Systems and Robotics (ASR) Area in the Intelligent Systems Division at the NASA Ames Research Center. VW has been publicly released under the terms of the NASA Open Source Software Agreement.&lt;/p&gt;

&lt;p&gt;The Vision Workbench was implemented in the C++ programming language and makes extensive use of C++ templates and generative programming techniques for conciseness of expression, efficiency of operation, and generalization of implementation.&lt;/p&gt;

&lt;p&gt;I suggest for everyone who works in computer vision to look at the source code of this library. The design of this library is very unusual - you will not find a direct memory
access there. Instead, all algorithms uses iterators, locators and templates. This produce a clean and very self-explanatory code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;template &amp;lt;class SrcT, class DestT&amp;gt;
void convolve_1d( SrcT const&amp;amp; src, DestT const&amp;amp; dest, std::vector&amp;lt;KernelT&amp;gt; const&amp;amp; kernel ) const {
  typedef typename SrcT::pixel_accessor SrcAccessT;
  typedef typename DestT::pixel_accessor DestAccessT;
  typedef typename DestT::pixel_type DestPixelT;
  typedef typename CompoundChannelType&amp;lt;DestPixelT&amp;gt;::type channel_type;

  VW_ASSERT( src.planes() == dest.planes(), ArgumentErr() &amp;lt;&amp;lt; &amp;quot;convolve_1d: Images should have the same number of planes&amp;quot; );

  SrcAccessT splane = src.origin();
  DestAccessT dplane = dest.origin();
  for( int32 p=0; p&amp;lt;dest.planes(); ++p ) {
    SrcAccessT srow = splane;
    DestAccessT drow = dplane;
    for( int32 y=0; y&amp;lt;dest.rows(); ++y ) {
      SrcAccessT scol = srow;
      DestAccessT dcol = drow;
      for( int32 x=0; x&amp;lt;dest.cols(); ++x ) {
        *dcol = channel_cast_clamp_if_int&amp;lt;channel_type&amp;gt;( correlate_1d_at_point( scol, kernel.rbegin(), kernel.size() ) );
        scol.next_col();
        dcol.next_col();
      }
      srow.next_row();
      drow.next_row();
    }
    splane.next_plane();
    dplane.next_plane();
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I recommend to take a look in this library to improve your language skills and have look on alternative approach how computer vision library can looks like.&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=&#34;https://github.com/nasa/visionworkbench&#34;&gt;nasa/visionworkbench&lt;/a&gt;.
&lt;hr /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;4&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;easy-optimization-of-image-processing-pipelines-using-decoupling-algorithms&#34;&gt;Easy optimization of image processing pipelines using decoupling algorithms&lt;/h2&gt;

&lt;p&gt;Take a look on picture below. Which code you find easier to percept? Both produce identical results, they have equal speed. But which one is easier to read and understand?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;halide_vs_cpp.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Very often I find myself thinking about how bored I am with tuning function with SSE, NEON or Assembly language. Real-time image processing requires you to count every millisecond, so sometimes you have to optimize slow functions, change a pipeline to &amp;lsquo;fuse&amp;rsquo; results or modify data flow to ensure better data locality. SIMD, GPGPU are good, not doubts. But they does not provide a final solution to fundamental problem - in my opinion, imperative approach limit the way you implement particular algorithm.&lt;/p&gt;

&lt;p&gt;There is a great idea - to separate Algorithm from it&amp;rsquo;s Implementation. Why this matters?
 - Writing fast image processing pipelines is hard
 - C-parallelism + tiling + fusion are hard to write or automate
 - CUDA, OpenCL, shaders - data parallelism is easy, fusion is hard
 - BLAS, IPP, OpenCV, MKL - optimized kernels compose into inefficient pipelines (no fusion)&lt;/p&gt;

&lt;p&gt;Proposed solution: Decouple Algorithm from Schedule&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Algorithm&lt;/em&gt; defines &lt;em&gt;what&lt;/em&gt; is computed.
&lt;em&gt;Schedule&lt;/em&gt; defines &lt;em&gt;where&lt;/em&gt; and &lt;em&gt;when&lt;/em&gt; it&amp;rsquo;s computed.&lt;/p&gt;

&lt;p&gt;Such decoupling lets developers a to build pipelines easy by defining a pure functions that operates on data in easy and clean way. No need to worry on tiling, parallelism and fusion. From the other side it will let the compiler to generate fast code.&lt;/p&gt;

&lt;p&gt;I want present you Halide - the image processing language that let&amp;rsquo;s you to write highly efficient code with less headache. Just compare two implementations of&lt;/p&gt;

&lt;p&gt;Here is an example written in Halide:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;halide-blur3x3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Even without experience with Halide, it is more or less clear what this code does. The Algorithm is separated from Schedule in very elegant way. During compilation stage,
Halide compiler will generate C++ code for particular platform.&lt;/p&gt;

&lt;h3 id=&#34;presentation-slides-on-halide-language&#34;&gt;Presentation slides on Halide language:&lt;/h3&gt;

&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;50622b92b4c3d10002018c4b&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt; 

&lt;p&gt;Original article: &lt;a href=&#34;http://people.csail.mit.edu/jrk/halide12/&#34;&gt;Decoupling algorithms from schedules for easy optimization of image processing pipelines&lt;/a&gt;.
&lt;hr /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;5&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;opencv-apparel-store&#34;&gt;OpenCV Apparel Store&lt;/h2&gt;

&lt;p&gt;This is less technical topic for the end of this digest. If you feel ok to support OpenCV open-source library - this one is for you.
What about having a T-shirt or hoodie with OpenCV logo?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;l26503.jpg&#34; alt=&#34;OpenCV Hoodie&#34; /&gt;
&lt;img src=&#34;lst30665.jpg&#34; alt=&#34;OpenCV Hoodie&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A nice way to say &amp;ldquo;thanks&amp;rdquo; to OpenCV team :)&lt;/p&gt;

&lt;p&gt;Store webpage: &lt;a href=&#34;http://fhstore.com/shopping/FHShop2.aspx?PON=74808&amp;amp;CON=75944&amp;amp;SCN=19&amp;amp;CN=76&amp;amp;ASN=&amp;amp;VSN=&amp;amp;AC=&#34;&gt;OpenCV Apparel Store&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-end&#34;&gt;The End&lt;/h2&gt;

&lt;p&gt;That&amp;rsquo;s all folks! I hope you enjoyed this digest. Please, leave your and feedbacks in comments. Please let me know if you interested in getting this digest on regular basis!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Training Haar cascade in the cloud</title>
      <link>/post/cloud-haartaining/</link>
      <pubDate>Tue, 20 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/cloud-haartaining/</guid>
      <description>

&lt;p&gt;In this post I&amp;rsquo;ll show you how you can train cascade classifier with OpenCV very quickly even if you have low-end hardware using virtual machine in the cloud.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;why-clouds&#34;&gt;Why Clouds?&lt;/h2&gt;

&lt;p&gt;Well, training a descriptor takes a lot of time. Depending on template size, number of samples and stages, it can take from several ours to a couple of days to train such cascade! Be prepared that during training stage your PC will likely be unusuable due to high RAM usage and CPU load. If you&amp;rsquo;re on laptop - it will become hot really quick. So, what if you have to train your cascade, but you don&amp;rsquo;t have either time or spare machine to do this?&lt;/p&gt;

&lt;p&gt;Recently I&amp;rsquo;ve faced this problem in one of my personal projects. What even more funny, a 10 hours flight was approaching, but I didn&amp;rsquo;t wanted to waste this time for nothing. I only had a laptop, but this task will drain my battery for sure. So I&amp;rsquo;ve decided to use virtual server to do this.&lt;/p&gt;

&lt;h2 id=&#34;step-1-environment-setup&#34;&gt;Step 1 - Environment setup&lt;/h2&gt;

&lt;p&gt;First, I&amp;rsquo;ve created a basic droplet in &lt;a href=&#34;https://www.digitalocean.com/?refcode=b93faa829f80&#34;&gt;DigitalOcean&lt;/a&gt;.
Yep, for 5$/month you can have your droplet that can do much!
It takes only 55 seconds to deploy a new instance (I assume you&amp;rsquo;re familiar with SSH keys, terminal, Git and so on.) and we&amp;rsquo;re ready to rock!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image1.png&#34; alt=&#34;Create DigitalOcean droplet&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;step-2-install-latest-opencv-release&#34;&gt;Step 2 - Install latest OpenCV release&lt;/h2&gt;

&lt;p&gt;There are two ways to do this: either using package managers (homebrew, yum or apt-get) or builiding it from scratch.
Personally I prefer second option since you can configure OpenCV. Usually I build static libs whith apps but without tests, java, cuda, python, OpenEXR, Jasper and Tiff. Regardless of the way you choose to install OpenCV, ensure that opencv apps (opencv_createsamples, opencv_traincascade) are also installed!.&lt;/p&gt;

&lt;h2 id=&#34;step-3-prepare-your-train-data&#34;&gt;Step 3 - Prepare your train data&lt;/h2&gt;

&lt;p&gt;There are a lot of tutorials &lt;a href=&#34;http://note.sonots.com/SciSoftware/haartraining.html&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html&#34;&gt;2&lt;/a&gt;, &lt;a href=&#34;http://answers.opencv.org/question/7141/about-traincascade-paremeters-samples-and-other/&#34;&gt;3&lt;/a&gt; on how to train cascade with OpenCV: which images are good for positive and negative samples and which settings should be used for cascade training. Let&amp;rsquo;s assume you have everything in a single folder on your load machine and there is a script called &amp;ldquo;train.sh&amp;rdquo; that starts training stage:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;opencv_traincascade -data classifier \
                    -vec &amp;lt;positive samples file&amp;gt; \
                    -bg &amp;lt;negative samples file&amp;gt; \
                    -numStages 12 \
                    -minHitRate 0.999 \
                    -maxFalseAlarmRate 0.5 \
                    -numPos 15000 -numNeg 17000 \
                    -w 24 \
                    -h 24 \
                    -mode ALL \
                    -nonsym 1 \
                    -featureType LBP
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-4-deploy-train-data-to-cloud&#34;&gt;Step 4 - Deploy train data to cloud&lt;/h2&gt;

&lt;p&gt;The easiest way to upload this folder to your virtual droplet is to use the &lt;a href=&#34;http://en.wikipedia.org/wiki/Rsync&#34;&gt;rsync&lt;/a&gt; tool.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rsync -avz &amp;lt;source&amp;gt; &amp;lt;destination&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For instance, the following command will upload the traindata/ folder with it&amp;rsquo;s content to ~haartraining.example.com~ webserver to /traindata directory. This example assumes that your public key has been added to haartraining.example.com during droplet creation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rsync -avz ~/Develop/traindata root@haartraining.example.com:/traindata
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-5-start-training&#34;&gt;Step 5 - Start training&lt;/h2&gt;

&lt;p&gt;The easiest way to execute training is to login to remote maching using ssh and execute the train script with a simple command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh root@haartraining.example.com
sh /traindata/train.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, this will require you to keep SSH-session open all the time. If you log-out, the process will terminate and training will be terminated as well.
To prevent this we can use &lt;a href=&#34;http://en.wikipedia.org/wiki/Nohup&#34;&gt;nohup&lt;/a&gt; UNIX utility:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh root@haartraining.example.com
nohup sh /traindata/train.sh &amp;gt; /traindata/train.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Training will continue to work regardless of the user&amp;rsquo;s connection to the terminal, and log results to the file train.log.&lt;/p&gt;

&lt;h2 id=&#34;step-6-getting-the-results&#34;&gt;Step 6 - Getting the results&lt;/h2&gt;

&lt;p&gt;After trainign is done (you can check this by top command output or looking at the train.log), we can download trainresults back with rsync command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rsync -avz root@haartraining.example.com:/traindata ~/Develop/traindata 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-7-speeding-up-training&#34;&gt;Step 7 - Speeding up training&lt;/h2&gt;

&lt;p&gt;To speed-up training stage I recommend to pass additional options to opencv_traincascade tool:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;precalcValBufSize=2048&lt;/li&gt;
&lt;li&gt;precalcIdxBufSize=2048&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ideally you want to use all available memory of your instance for these buffers, so if you have 4Gb of RAM installed, pass at least a 1Gb to each of these buffers.&lt;/p&gt;

&lt;p&gt;It also may be a good idea to &amp;ldquo;shrink&amp;rdquo; the DigitalOcean&amp;rsquo;s droplet to more powerful configuration which gives you 16Gb of RAM and 8 CPU&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;resize.png&#34; alt=&#34;Resize droplet&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This task can be done on few clicks:&lt;/p&gt;

&lt;h3 id=&#34;stop-your-droplet&#34;&gt;Stop your droplet&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;poweroff.png&#34; alt=&#34;Poweroff droplet&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;choose-resize-and-pick-a-necessary-configuration&#34;&gt;Choose &amp;ldquo;Resize&amp;rdquo; and pick a necessary configuration&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;resize2.png&#34; alt=&#34;Resize droplet&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;power-up-your-resized-droplet&#34;&gt;Power-up your resized droplet&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Please be advised, that DigitalOcean will charge you regardless whether your droplet is powered on or off. So if you&amp;rsquo;re not using it - make a snapshot of it and delete unused droplet to save money&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all. I hope you enjoyed reading this post. Please, leave your comments.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>